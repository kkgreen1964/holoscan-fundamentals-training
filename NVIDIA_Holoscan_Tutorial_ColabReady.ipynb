{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5c6fb7a0-a646-496d-95b5-3415c1d51471",
      "metadata": {
        "id": "5c6fb7a0-a646-496d-95b5-3415c1d51471"
      },
      "source": [
        "# NVIDIA Holoscan: Fundamentals & Quick Start Tutorial\n",
        "\n",
        "## What You'll Learn\n",
        "\n",
        "This hands-on tutorial introduces NVIDIA Holoscan, a powerful SDK designed for building high-performance, GPU-accelerated streaming AI applications.By the end of this tutorial, you'll understand how to build efficient data processing pipelines that leverage GPU acceleration for real-time AI inference.\n",
        "\n",
        "## Key Topics Covered\n",
        "\n",
        "- The role of Holoscan in addressing CPU bottlenecks in AI pipelines\n",
        "- Building custom operators for data processing\n",
        "- Creating efficient data flows between operators\n",
        "- Using pre-built Holoscan operators for common tasks\n",
        "- Implementing a complete face detection application\n",
        "- Optimizing video processing pipelines for real-time performance\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- Basic understanding of Python\n",
        "- Familiarity with GPU computing concepts\n",
        "- NVIDIA GPU hardware (for running examples)\n",
        "- NVIDIA Holoscan SDK installed\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Google Drive in Colab"
      ],
      "metadata": {
        "id": "9GXuzTNaGve-"
      },
      "id": "9GXuzTNaGve-"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "QfitxHBYG37d"
      },
      "id": "QfitxHBYG37d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Copy files from your Drive to Colab /content/scripts/ while preserving structure"
      ],
      "metadata": {
        "id": "Ta2ghnbNJ4Dv"
      },
      "id": "Ta2ghnbNJ4Dv"
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil\n",
        "\n",
        "# Define paths\n",
        "base_drive = '/content/drive/My Drive/Colab Notebooks'\n",
        "\n",
        "# Ensure folders exist in Colab\n",
        "os.makedirs('/content/scripts/ping', exist_ok=True)\n",
        "os.makedirs('/content/scripts/tao_peoplenet/data', exist_ok=True)\n",
        "os.makedirs('/content/images', exist_ok=True)\n",
        "\n",
        "# Copy scripts and video data\n",
        "shutil.copy(f'{base_drive}/scripts/ping/ping.yaml', '/content/scripts/ping/ping.yaml')\n",
        "shutil.copy(f'{base_drive}/scripts/tao_peoplenet/data/people.mp4', '/content/scripts/tao_peoplenet/data/people.mp4')\n",
        "\n",
        "# Copy images from Google Drive into Colab\n",
        "shutil.copy(f'{base_drive}/images/face_and_people_detection_app.png', '/content/images/face_and_people_detection_app.png')\n",
        "shutil.copy(f'{base_drive}/images/MyPingApp.png', '/content/images/MyPingApp.png')"
      ],
      "metadata": {
        "id": "i5l__oGuie0W"
      },
      "id": "i5l__oGuie0W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment Initialization"
      ],
      "metadata": {
        "id": "lI5_HSkBCtR_"
      },
      "id": "lI5_HSkBCtR_"
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚öôÔ∏è Environment Initialization for CUDA, PyCUDA & Modern NumPy\n",
        "\n",
        "import os, ctypes\n",
        "\n",
        "# 1) Upgrade to a modern NumPy that satisfies JAX/Torch/etc.\n",
        "!pip install numpy==1.25.2 --quiet --force-reinstall\n",
        "\n",
        "# 2) Monkey-patch the old alias PyCUDA and Holoscan expect\n",
        "import numpy as np\n",
        "np.bool8 = np.bool_\n",
        "\n",
        "# 3) (Re-)install PyCUDA so it picks up our numpy patch\n",
        "!pip install pycuda --quiet --force-reinstall\n",
        "\n",
        "# 4) Point at your CUDA libs and force-load the driver\n",
        "os.environ[\"LD_LIBRARY_PATH\"] = (\n",
        "    \"/usr/local/cuda/lib64:\"\n",
        "    \"/usr/local/cuda/compat:\"\n",
        "    \"/usr/local/nvidia/lib:\"\n",
        "    \"/usr/local/nvidia/lib64\"\n",
        ")\n",
        "ctypes.CDLL(\"libcuda.so.1\")  # should print nothing on success\n",
        "print(\"‚úÖ CUDA driver loaded, NumPy patched to\", np.__version__)"
      ],
      "metadata": {
        "id": "0R_8kyVub9ew"
      },
      "id": "0R_8kyVub9ew",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyCUDA Verification"
      ],
      "metadata": {
        "id": "_ug8i9GdC3lR"
      },
      "id": "_ug8i9GdC3lR"
    },
    {
      "cell_type": "code",
      "source": [
        "# üîç Test PyCUDA functionality & confirm monkey-patch\n",
        "\n",
        "import numpy as np\n",
        "print(\"NumPy version:\", np.__version__, \"| has bool8:\", hasattr(np, \"bool8\"))\n",
        "\n",
        "try:\n",
        "    import pycuda.driver as cuda\n",
        "    import pycuda.autoinit\n",
        "    print(\"‚úÖ PyCUDA loaded and initialized.\")\n",
        "    print(\"GPU Detected:\", cuda.Device(0).name())\n",
        "except Exception as e:\n",
        "    print(\"‚ö†Ô∏è PyCUDA not available:\", e)"
      ],
      "metadata": {
        "id": "V-4w8ZUvcFlW"
      },
      "id": "V-4w8ZUvcFlW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "3ddf6368-389c-4887-a259-eb4efbca0d73",
      "metadata": {
        "id": "3ddf6368-389c-4887-a259-eb4efbca0d73"
      },
      "source": [
        "## Table Styling for Left Alignment\n",
        "\n",
        "**Execute this cell once to ensure proper table formatting throughout the notebook**\n",
        "\n",
        "This CSS injection overrides Jupyter's default table centering behavior, ensuring all tables align naturally with surrounding text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19a26c24-98b2-4844-a3c0-eaababc8d288",
      "metadata": {
        "id": "19a26c24-98b2-4844-a3c0-eaababc8d288"
      },
      "outputs": [],
      "source": [
        "# Import necessary dependencies\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "display(HTML(\"\"\"\n",
        "<style>\n",
        "table, th, td {\n",
        "    text-align: left !important;\n",
        "    margin-left: 0 !important;\n",
        "}\n",
        "</style>\n",
        "\"\"\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary dependencies\n",
        "from IPython.display import Markdown, Image\n",
        "\n",
        "# Display markdown text and images\n",
        "display(Markdown(\"\"\"\n",
        "# Background & Motivation: Understanding the Data Pipeline\n",
        "\n",
        "## The Data Flow Challenge\n",
        "In deep learning workflows, data typically follows this path:\n",
        "```\n",
        "Storage ‚Üí CPU Memory ‚Üí (Optional) GPU Memory ‚Üí Processing ‚Üí Inference\n",
        "```\n",
        "\n",
        "This seemingly straightforward pipeline often becomes a major bottleneck in AI/ML applications, limiting overall system performance.\n",
        "\n",
        "## CPU Bottlenecks in AI/ML Processing Pipelines\n",
        "1. **Data Loading**: Pre- and post-processing tasks account for roughly 90% of the workload.\n",
        "2. **Processing Overhead**: The cost of these operations varies by task, with image preprocessing being particularly intensive.\n",
        "3. **Library Impact**: Common Python libraries, such as Torch or Keras, can inadvertently introduce CPU bottlenecks.\n",
        "\n",
        "*Conceptual Diagram: AI Imaging Pipeline in Cloud/Data Center Environments*\n",
        "\"\"\"))\n",
        "\n",
        "# First diagram - CPU bottlenecks\n",
        "display(Image('/content/drive/My Drive/Colab Notebooks/CPU_Bottlenecks_in_AI_ML_Pipelines-All_CPU.jpg'))\n",
        "\n",
        "display(Markdown(\"\"\"\n",
        "## GPU Acceleration using Accelerator Frameworks\n",
        "\"\"\"))\n",
        "\n",
        "# Second diagram - GPU acceleration\n",
        "display(Image('/content/drive/My Drive/Colab Notebooks/CPU_Bottlenecks_in_AI_ML_Pipelines-All_GPU.jpg'))\n",
        "\n",
        "display(Markdown(\"\"\"\n",
        "Moving all intermediate preprocessing stages from CPU to GPU delivers significant benefits:\n",
        "- **>10x throughput improvement** by keeping the entire pipeline on the same GPU\n",
        "- **Significant cloud cost savings** with accelerated software\n",
        "- **Reduced memory requirements** by eliminating intermediate storage products\n",
        "\n",
        "## NVIDIA Holoscan: A Comparative Advantage\n",
        "Holoscan addresses common limitations in streaming AI workflows through several key improvements:\n",
        "\n",
        "### Traditional Approaches vs. Holoscan\n",
        "| Traditional Approaches          | NVIDIA Holoscan                   |\n",
        "|:--------------------------------|:----------------------------------|\n",
        "| CPU bottlenecks and high latency | Specialized streaming architecture |\n",
        "| Independent pipeline components | End-to-end pipeline optimization  |\n",
        "| Complex I/O integration         | Seamless GPU acceleration         |\n",
        "| Manual hardware optimization    | Unified sensor-to-insight stack   |\n",
        "\n",
        "### General AI Frameworks vs. Holoscan\n",
        "| General AI Frameworks           | NVIDIA Holoscan                   |\n",
        "|:--------------------------------|:----------------------------------|\n",
        "| Optimized for batch processing  | Designed for streaming data       |\n",
        "| Limited real-time guarantees    | Deterministic low latency         |\n",
        "| Separate tools for deployment   | End-to-end solution               |\n",
        "| Training-focused design         | Inference-optimized architecture  |\n",
        "\n",
        "## Key Differentiators\n",
        "NVIDIA Holoscan is uniquely positioned for applications where milliseconds matter:\n",
        "- **Purpose-built for time-critical streaming sensor workflows**\n",
        "- **Pre-optimized operators for common signal and image processing tasks**\n",
        "- **Scale from the edge (embedd\n",
        "ed Jetson) to the datacenter (DGX) seamlessly with the same code**\n",
        "- **Domain-specific adaptability with 90+ reference applications**\n",
        "\n",
        "These advantages make Holoscan particularly valuable for applications like surgical robotics, radio astronomy, autonomous systems, and real-time monitoring solutions.\n",
        "\n",
        "In the following sections, we will explore how to leverage these advantages by building applications with Holoscan, starting with a basic understanding of the system architecture and then progressing to hands-on development.\n",
        "\"\"\"))"
      ],
      "metadata": {
        "id": "DVKk_LsiMNVw"
      },
      "id": "DVKk_LsiMNVw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "75e75196-97e5-49c2-beb3-403f74ecee70",
      "metadata": {
        "id": "75e75196-97e5-49c2-beb3-403f74ecee70"
      },
      "source": [
        "<!-- # Background & Motivation: Understanding the Data Pipeline\n",
        "\n",
        "## The Data Flow Challenge\n",
        "\n",
        "In deep learning workflows, data typically follows this path:\n",
        "\n",
        "```\n",
        "Storage ‚Üí CPU Memory ‚Üí (Optional) GPU Memory ‚Üí Processing ‚Üí Inference\n",
        "```\n",
        "\n",
        "This seemingly straightforward pipeline often becomes a major bottleneck in AI/ML applications, limiting overall system performance.\n",
        "\n",
        "## CPU Bottlenecks in AI/ML Processing Pipelines\n",
        "\n",
        "1. **Data Loading**: Pre- and post-processing tasks account for roughly 90% of the workload.\n",
        "2. **Processing Overhead**: The cost of these operations varies by task, with image preprocessing being particularly intensive.\n",
        "3. **Library Impact**: Common Python libraries, such as Torch or Keras, can inadvertently introduce CPU bottlenecks.\n",
        "\n",
        "*Conceptual Diagram: AI Imaging Pipeline in Cloud/Data Center Environments*\n",
        "\n",
        "![CPU Bottlenecks](CPU_Bottlenecks_in_AI_ML_Pipelines-All_CPU.jpg)\n",
        "\n",
        "## GPU Acceleration using Accelerator Frameworks\n",
        "\n",
        "![GPU Acceleration](CPU_Bottlenecks_in_AI_ML_Pipelines-All_GPU.jpg)\n",
        "\n",
        "Moving all intermediate preprocessing stages from CPU to GPU delivers significant benefits:\n",
        "\n",
        "- **>10x throughput improvement** by keeping the entire pipeline on the same GPU\n",
        "- **Significant cloud cost savings** with accelerated software\n",
        "- **Reduced memory requirements** by eliminating intermediate storage products\n",
        "\n",
        "## NVIDIA Holoscan: A Comparative Advantage\n",
        "\n",
        "Holoscan addresses common limitations in streaming AI workflows through several key improvements:\n",
        "\n",
        "### Traditional Approaches vs. Holoscan\n",
        "\n",
        "| Traditional Approaches          | NVIDIA Holoscan                   |\n",
        "|:--------------------------------|:----------------------------------|\n",
        "| CPU bottlenecks and high latency | Specialized streaming architecture |\n",
        "| Independent pipeline components | End-to-end pipeline optimization  |\n",
        "| Complex I/O integration         | Seamless GPU acceleration         |\n",
        "| Manual hardware optimization    | Unified sensor-to-insight stack   |\n",
        "\n",
        "### General AI Frameworks vs. Holoscan\n",
        "\n",
        "| General AI Frameworks           | NVIDIA Holoscan                   |\n",
        "|:--------------------------------|:----------------------------------|\n",
        "| Optimized for batch processing  | Designed for streaming data       |\n",
        "| Limited real-time guarantees   | Deterministic low latency         |\n",
        "| Separate tools for deployment  | End-to-end solution               |\n",
        "| Training-focused design        | Inference-optimized architecture  |\n",
        "\n",
        "## Key Differentiators\n",
        "\n",
        "NVIDIA Holoscan is uniquely positioned for applications where milliseconds matter:\n",
        "\n",
        "- **Purpose-built for time-critical streaming sensor workflows**\n",
        "- **Pre-optimized operators for common signal and image processing tasks**\n",
        "- **Scale from the edge (embedded Jetson) to the datacenter (DGX) seamlessly with the same code**\n",
        "- **Domain-specific adaptability with 90+ reference applications**\n",
        "\n",
        "These advantages make Holoscan particularly valuable for applications like surgical robotics, radio astronomy, autonomous systems, and real-time monitoring solutions.\n",
        "\n",
        "In the following sections, we will explore how to leverage these advantages by building applications with Holoscan, starting with a basic understanding of the system architecture and then progressing to hands-on development. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "294988dc-583c-4cbd-a84b-3a8d56e71e19",
      "metadata": {
        "id": "294988dc-583c-4cbd-a84b-3a8d56e71e19"
      },
      "source": [
        "## Installing Required Google Colab Python and System Packages\n",
        "\n",
        "Before we start building Holoscan applications, let's set up our environment and install the necessary dependencies so that it will run successfully in Google Colab.\n",
        "\n",
        "We will begin by installing all of the required packages to that this notebook will run succesfully in Google Colab:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üõ†Ô∏è ENVIRONMENT SETUP FOR NVIDIA HOLOSCAN IN GOOGLE COLAB\n",
        "\n",
        "# Step 1: Write out a minimal requirements file ===\n",
        "%%writefile cleaned_requirements.txt\n",
        "anyio==4.9.0\n",
        "appdirs==1.4.4\n",
        "argon2-cffi==23.1.0\n",
        "argon2-cffi-bindings==21.2.0\n",
        "arrow==1.3.0\n",
        "asttokens==3.0.0\n",
        "async-lru==2.0.5\n",
        "attrs==25.3.0\n",
        "babel==2.17.0\n",
        "beautifulsoup4==4.13.4\n",
        "bleach==6.2.0\n",
        "certifi==2024.7.4\n",
        "cffi==1.17.1\n",
        "charset-normalizer==3.3.2\n",
        "click==8.1.7\n",
        "cloudpickle==2.2.1\n",
        "comm==0.2.2\n",
        "cupy-cuda12x==12.0.0\n",
        "debugpy==1.8.14\n",
        "decorator==5.2.1\n",
        "defusedxml==0.7.1\n",
        "exceptiongroup==1.2.2\n",
        "executing==2.2.0\n",
        "fastjsonschema==2.21.1\n",
        "fastrlock==0.8.2\n",
        "fqdn==1.5.1\n",
        "h11==0.16.0\n",
        "httpcore==1.0.9\n",
        "httpx==0.28.1\n",
        "idna==3.7\n",
        "ipython==8.36.0\n",
        "isoduration==20.11.0\n",
        "jedi==0.19.2\n",
        "Jinja2==3.1.3\n",
        "json5==0.12.0\n",
        "jsonpointer==3.0.0\n",
        "jsonschema==4.23.0\n",
        "jsonschema-specifications==2025.4.1\n",
        "Mako==1.2.4\n",
        "markdown-it-py==3.0.0\n",
        "MarkupSafe==2.1.3\n",
        "matplotlib-inline==0.1.7\n",
        "mdurl==0.1.2\n",
        "mistune==3.1.3\n",
        "nbclient==0.10.2\n",
        "nbconvert==7.16.6\n",
        "nbformat==5.10.4\n",
        "nest-asyncio==1.6.0\n",
        "overrides==7.7.0\n",
        "packaging==23.1\n",
        "pandocfilters==1.5.1\n",
        "parso==0.8.4\n",
        "pexpect==4.9.0\n",
        "platformdirs==3.10.0\n",
        "polygraphy==0.49.0\n",
        "prometheus_client==0.21.1\n",
        "prompt_toolkit==3.0.51\n",
        "protobuf==4.23.4\n",
        "psutil==5.9.6\n",
        "ptyprocess==0.7.0\n",
        "pure_eval==0.2.3\n",
        "pycparser==2.22\n",
        "pydantic==1.10.17\n",
        "Pygments==2.18.0\n",
        "python-dateutil==2.9.0.post0\n",
        "python-json-logger==3.3.0\n",
        "pytools==2023.1.1\n",
        "PyYAML==6.0\n",
        "pyzmq==26.4.0\n",
        "referencing==0.36.2\n",
        "requests==2.31.0\n",
        "rfc3339-validator==0.1.4\n",
        "rfc3986-validator==0.1.1\n",
        "rich==13.7.1\n",
        "rpds-py==0.24.0\n",
        "Send2Trash==1.8.3\n",
        "shellingham==1.5.4\n",
        "six==1.17.0\n",
        "sniffio==1.3.1\n",
        "soupsieve==2.7\n",
        "stack-data==0.6.3\n",
        "terminado==0.18.1\n",
        "tinycss2==1.4.0\n",
        "tomli==2.2.1\n",
        "tqdm==4.66.4\n",
        "typer==0.12.3\n",
        "types-python-dateutil==2.9.0.20241206\n",
        "typing_extensions==4.7.1\n",
        "uri-template==1.3.0\n",
        "urllib3==2.2.2\n",
        "wcwidth==0.2.13\n",
        "webcolors==24.11.1\n",
        "webencodings==0.5.1\n",
        "websocket-client==1.8.0\n",
        "wheel-axle-runtime==0.0.6"
      ],
      "metadata": {
        "id": "iTka5lTQ9b_f"
      },
      "id": "iTka5lTQ9b_f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Install system dependencies\n",
        "!apt-get update -qq\n",
        "!apt-get install --no-install-recommends -y ffmpeg libegl1"
      ],
      "metadata": {
        "id": "1W8LX0Lm-pKR"
      },
      "id": "1W8LX0Lm-pKR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Install from requirements and additional useful libraries\n",
        "!pip install -q --no-deps -r cleaned_requirements.txt opencv-python matplotlib"
      ],
      "metadata": {
        "id": "2YT-nioU-sxs"
      },
      "id": "2YT-nioU-sxs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: NVIDIA SDK support\n",
        "!pip install --no-deps nvidia-pyindex\n",
        "!pip install --no-deps tensorrt\n",
        "!pip install holoscan"
      ],
      "metadata": {
        "id": "d5dDdyPD-tYk"
      },
      "id": "d5dDdyPD-tYk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ IMPORT VALIDATION\n",
        "print(\"\\nüîç Validating critical imports...\\n\")\n",
        "\n",
        "# Standard libs\n",
        "try:\n",
        "    import numpy as np\n",
        "    import matplotlib.pyplot as plt\n",
        "    import cv2\n",
        "    print(\"‚úÖ numpy, matplotlib, OpenCV\")\n",
        "except Exception as e:\n",
        "    print(\"‚ùå Standard libs failed:\", e)\n",
        "\n",
        "# Holoscan\n",
        "try:\n",
        "    import holoscan\n",
        "    print(\"‚úÖ Holoscan SDK\")\n",
        "except Exception as e:\n",
        "    print(\"‚ùå Holoscan failed:\", e)\n",
        "\n",
        "# TensorRT\n",
        "try:\n",
        "    import tensorrt as trt\n",
        "    print(\"‚úÖ TensorRT\")\n",
        "except Exception as e:\n",
        "    print(\"‚ö†Ô∏è TensorRT failed:\", e)\n",
        "\n",
        "# PyCUDA\n",
        "try:\n",
        "    import pycuda.driver as cuda\n",
        "    import pycuda.autoinit\n",
        "    print(\"‚úÖ PyCUDA\")\n",
        "except Exception as e:\n",
        "    print(\"‚ö†Ô∏è PyCUDA failed:\", e)"
      ],
      "metadata": {
        "id": "58U669Kn-3PY"
      },
      "id": "58U669Kn-3PY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "72fd6d6b-8c59-441b-a7ce-be4a99dbb616",
      "metadata": {
        "id": "72fd6d6b-8c59-441b-a7ce-be4a99dbb616"
      },
      "source": [
        "## Importing Holoscan Core Classes\n",
        "\n",
        "Now, let's import the necessary classes from the Holoscan SDK:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4009f961-63aa-4421-89d6-0aebb41ef38a",
      "metadata": {
        "id": "4009f961-63aa-4421-89d6-0aebb41ef38a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import time\n",
        "import logging\n",
        "import threading\n",
        "import subprocess\n",
        "import numpy as np\n",
        "from holoscan.core import Operator, OperatorSpec\n",
        "\n",
        "# Set up logging configuration early in your script\n",
        "def configure_logging():\n",
        "    # Configure root logger\n",
        "    logging.getLogger().setLevel(logging.WARNING)  # Set overall level to WARNING to suppress INFO messages\n",
        "\n",
        "    # You can also specifically target the video_stream_replayer module if needed\n",
        "    logging.getLogger('video_stream_replayer').setLevel(logging.WARNING)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6315c39-f9c8-4506-896b-83937b624a6d",
      "metadata": {
        "id": "f6315c39-f9c8-4506-896b-83937b624a6d"
      },
      "source": [
        "## Understanding Holoscan's Architecture\n",
        "\n",
        "Holoscan is built around a few core concepts:\n",
        "\n",
        "1. **Operators**: The basic processing units that perform specific functions (e.g., data loading, inference, visualization)\n",
        "2. **Data Flow**: Defines how data moves between operators\n",
        "3. **Application**: Orchestrates the operators and data flow\n",
        "4. **Scheduler**: Manages the execution of operators\n",
        "\n",
        "The `ValueData` class is one of the fundamental building blocks for passing data between operators:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca44415b-d402-4a89-886b-a696957085a8",
      "metadata": {
        "id": "ca44415b-d402-4a89-886b-a696957085a8"
      },
      "outputs": [],
      "source": [
        "class ValueData:\n",
        "    \"\"\"Example of a custom Python class\"\"\"\n",
        "\n",
        "    def __init__(self, value):\n",
        "        self.data = value\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"ValueData({self.data})\"\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        return self.data == other.data\n",
        "\n",
        "    def __hash__(self):\n",
        "        return hash(self.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4395fc31-8873-4d6a-96f0-c42573fcf5de",
      "metadata": {
        "id": "4395fc31-8873-4d6a-96f0-c42573fcf5de"
      },
      "source": [
        "This class serves several important functions in a Holoscan application:\n",
        "\n",
        "- **Data Encapsulation**: Provides a structured way to encapsulate data values\n",
        "- **Inter-Operator Communication**: Enables communication between different operators\n",
        "- **Type Safety**: Ensures consistent data types throughout the processing pipeline\n",
        "- **Memory Management**: Facilitates proper memory handling between operators\n",
        "\n",
        "With our environment set up, we're ready to build our first Holoscan application in the next section."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5caf9e81-f4ab-40ab-ac0c-28312af4eb4e",
      "metadata": {
        "id": "5caf9e81-f4ab-40ab-ac0c-28312af4eb4e"
      },
      "source": [
        "# Our First Holoscan Application\n",
        "\n",
        "For our first Holoscan application, we will create a simple data processing pipeline that demonstrates the core concepts of operators and data flow."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import display dependencies\n",
        "from IPython.display import Markdown, Image, display\n",
        "\n",
        "# Display the application overview text\n",
        "display(Markdown(\"\"\"\n",
        "# Application Overview\n",
        "\n",
        "We'll build a pipeline with three operators:\n",
        "\n",
        "1. **PingTxOp** ‚Äì a source operator that generates integers and emits them via two output ports\n",
        "2. **PingMiddleOp** ‚Äì a processing operator that receives the integers, multiplies them by a configurable value, and emits the results\n",
        "3. **PingRxOp** ‚Äì a sink operator that receives and displays the processed values\n",
        "\n",
        "Below is a diagram of our application pipeline:\n",
        "\"\"\"))\n",
        "\n",
        "# Display the application diagram image\n",
        "display(Image(filename='/content/images/MyPingApp.png', width=1000))"
      ],
      "metadata": {
        "id": "8kqBqFDOjuqH"
      },
      "id": "8kqBqFDOjuqH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "8dff90bb-a8fa-4629-a0d7-21e6955a7f01",
      "metadata": {
        "id": "8dff90bb-a8fa-4629-a0d7-21e6955a7f01"
      },
      "source": [
        "## Creating Custom Operators\n",
        "\n",
        "Let's implement each of these operators step by step.\n",
        "\n",
        "### 1. Source Operator: PingTxOp\n",
        "\n",
        "This operator generates sequential integer values encapsulated in ValueData objects and emits them through two output ports, \"out1\" and \"out2\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8233411b-035e-4a5a-ac57-a89621040d76",
      "metadata": {
        "id": "8233411b-035e-4a5a-ac57-a89621040d76"
      },
      "outputs": [],
      "source": [
        "class PingTxOp(Operator):\n",
        "    \"\"\"Simple transmitter operator.\n",
        "    This operator has:\n",
        "        outputs: \"out1\", \"out2\"\n",
        "    On each tick, it transmits a `ValueData` object at each port. The\n",
        "    transmitted values are even on port1 and odd on port2 and increment with\n",
        "    each call to compute.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        self.index = 0\n",
        "        # Need to call the base class constructor last\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def setup(self, spec: OperatorSpec):\n",
        "        spec.output(\"out1\")\n",
        "        spec.output(\"out2\")\n",
        "\n",
        "    def compute(self, op_input, op_output, context):\n",
        "        value1 = ValueData(self.index)\n",
        "        self.index += 1\n",
        "        op_output.emit(value1, \"out1\")\n",
        "\n",
        "        value2 = ValueData(self.index)\n",
        "        self.index += 1\n",
        "        op_output.emit(value2, \"out2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a25cdcc-598f-40fb-800c-9ae4c8a7fef9",
      "metadata": {
        "id": "1a25cdcc-598f-40fb-800c-9ae4c8a7fef9"
      },
      "source": [
        "### 2. Processing Operator: PingMiddleOp\n",
        "\n",
        "This operator receives integer values through two input ports, multiplies them by a configurable parameter, and emits the processed results through two output ports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29c9893f-b7dd-4b29-8628-00aef1beeead",
      "metadata": {
        "id": "29c9893f-b7dd-4b29-8628-00aef1beeead"
      },
      "outputs": [],
      "source": [
        "class PingMiddleOp(Operator):\n",
        "    \"\"\"Example of an operator modifying data.\n",
        "    This operator has:\n",
        "        inputs:  \"in1\", \"in2\"\n",
        "        outputs: \"out1\", \"out2\"\n",
        "    The data from each input is multiplied by a user-defined value.\n",
        "    In this demo, the `multiplier` parameter value is read from a \"ping.yaml\"\n",
        "    configuration file (near the bottom of this script), overriding the default\n",
        "    defined in the setup() method below.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        # If `self.multiplier` is set here (e.g., `self.multiplier = 4`), then\n",
        "        # the default value by `param()` in `setup()` will be ignored.\n",
        "        # (you can just call `spec.param(\"multiplier\")` in `setup()` to use the\n",
        "        # default value)\n",
        "\n",
        "        # self.multiplier = 4\n",
        "        self.count = 1\n",
        "\n",
        "        # Need to call the base class constructor last\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def setup(self, spec: OperatorSpec):\n",
        "        spec.input(\"in1\")\n",
        "        spec.input(\"in2\")\n",
        "        spec.output(\"out1\")\n",
        "        spec.output(\"out2\")\n",
        "        spec.param(\"multiplier\", 2)\n",
        "\n",
        "    def compute(self, op_input, op_output, context):\n",
        "        value1 = op_input.receive(\"in1\")\n",
        "        value2 = op_input.receive(\"in2\")\n",
        "        print(f\"Middle message received (count: {self.count})\")\n",
        "        self.count += 1\n",
        "\n",
        "        print(f\"Middle message value1: {value1.data}\")\n",
        "        print(f\"Middle message value2: {value2.data}\")\n",
        "\n",
        "        # Multiply the values by the multiplier parameter\n",
        "        value1.data *= self.multiplier\n",
        "        value2.data *= self.multiplier\n",
        "\n",
        "        op_output.emit(value1, \"out1\")\n",
        "        op_output.emit(value2, \"out2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cc15375-9548-4489-8ddb-386943d9c785",
      "metadata": {
        "id": "3cc15375-9548-4489-8ddb-386943d9c785"
      },
      "source": [
        "### 3. Sink Operator: PingRxOp\n",
        "\n",
        "This sink operator receives multiple input values through its special \"receivers\" port that can accept multiple connections, collects them into a single array, and simply prints the received values without emitting any outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e92c9523-62ee-41d2-bce1-cad8fc4e8ad2",
      "metadata": {
        "id": "e92c9523-62ee-41d2-bce1-cad8fc4e8ad2"
      },
      "outputs": [],
      "source": [
        "class PingRxOp(Operator):\n",
        "    \"\"\"Simple receiver operator.\n",
        "    This operator has:\n",
        "        input: \"receivers\"\n",
        "    This is an example of a native operator that can dynamically have any\n",
        "    number of inputs connected to is \"receivers\" port.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        self.count = 1\n",
        "        # Need to call the base class constructor last\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def setup(self, spec: OperatorSpec):\n",
        "        spec.param(\"receivers\", kind=\"receivers\")\n",
        "\n",
        "    def compute(self, op_input, op_output, context):\n",
        "        values = op_input.receive(\"receivers\")\n",
        "        print(f\"Rx message received (count: {self.count}, size: {len(values)})\")\n",
        "        self.count += 1\n",
        "        print(f\"Rx message value1: {values[0].data}\")\n",
        "        print(f\"Rx message value2: {values[1].data}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71d2abd2-f35e-4e0a-8fda-f098ed1d4b96",
      "metadata": {
        "id": "71d2abd2-f35e-4e0a-8fda-f098ed1d4b96"
      },
      "source": [
        "## Anatomy of a Holoscan Operator\n",
        "\n",
        "Each Holoscan operator follows a consistent structure:\n",
        "\n",
        "1. **`__init__` Method**: Initializes the operator and its parameters\n",
        "2. **`setup` Method**: Defines the operator's input and output ports\n",
        "3. **`compute` Method**: Implements the operator's processing logic\n",
        "\n",
        "The `compute` method is where the actual data processing happens:\n",
        "- `op_input.receive(port_name)` receives data from an input port\n",
        "- `op_output.emit(data, port_name)` sends data to an output port"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c481e355-96f2-4a9e-8884-7317488579cf",
      "metadata": {
        "id": "c481e355-96f2-4a9e-8884-7317488579cf"
      },
      "source": [
        "## Connecting Operators to Form an Application\n",
        "\n",
        "Now that we have our operators, let's connect them to form a complete application:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60f186e0-16fb-4d0a-a49c-588a0a8a7baa",
      "metadata": {
        "id": "60f186e0-16fb-4d0a-a49c-588a0a8a7baa"
      },
      "outputs": [],
      "source": [
        "from holoscan.conditions import CountCondition\n",
        "from holoscan.core import Application\n",
        "\n",
        "class MyPingApp(Application):\n",
        "    def compose(self):\n",
        "        # Configure the operators. Here we use CountCondition to terminate\n",
        "        # execution after a specific number of messages have been sent.\n",
        "        tx = PingTxOp(self, CountCondition(self, 10), name=\"tx\")\n",
        "        mx = PingMiddleOp(self, self.from_config(\"mx\"), name=\"mx\")\n",
        "        rx = PingRxOp(self, name=\"rx\")\n",
        "\n",
        "        # Connect the operators into the workflow:  tx -> mx -> rx\n",
        "        self.add_flow(tx, mx, {(\"out1\", \"in1\"), (\"out2\", \"in2\")})\n",
        "        self.add_flow(mx, rx, {(\"out1\", \"receivers\"), (\"out2\", \"receivers\")})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cf4a7c4-e7ef-4d16-9332-36c5d449f766",
      "metadata": {
        "id": "8cf4a7c4-e7ef-4d16-9332-36c5d449f766"
      },
      "source": [
        "In the `compose` method, we:\n",
        "1. Create instances of our three operators\n",
        "2. Configure the source operator to run 10 times using `CountCondition`\n",
        "3. Connect the operators using `add_flow` to define the data flow between them"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aec7ce4b-a4a2-4116-8f56-565af383e432",
      "metadata": {
        "id": "aec7ce4b-a4a2-4116-8f56-565af383e432"
      },
      "source": [
        "## Running the Application\n",
        "\n",
        "To run the application, we need to create an instance of our application class, configure it with a YAML file if needed, and call its run method, which will execute our ping application and process 10 pairs of values through the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2702c75e-a404-4bbd-afec-bb2e58866c76",
      "metadata": {
        "id": "2702c75e-a404-4bbd-afec-bb2e58866c76"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    app = MyPingApp()\n",
        "    app.config(\"./scripts/ping/ping.yaml\") # Optional configuration file\n",
        "    app.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cd1fe2f-c70c-44f0-8a2f-9fd73c81e59b",
      "metadata": {
        "id": "5cd1fe2f-c70c-44f0-8a2f-9fd73c81e59b"
      },
      "source": [
        "## Understanding Key Concepts\n",
        "\n",
        "Let's review some key concepts we've covered:\n",
        "\n",
        "1. **Operators**: The basic building blocks of a Holoscan application\n",
        "2. **Ports**: Define how data flows between operators\n",
        "3. **Data Flow**: Specifies the connections between operators\n",
        "4. **Application**: Orchestrates the operators and data flow\n",
        "5. **Conditions**: Control when and how often operators execute\n",
        "\n",
        "In the next section, we'll explore more advanced operators and build a more complex application for real-time video processing for detecting faces."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4dc87971-4f16-4278-8a9d-f43e0ca71d0a",
      "metadata": {
        "id": "4dc87971-4f16-4278-8a9d-f43e0ca71d0a"
      },
      "source": [
        "# Optional Exercises: Building a Custom Operator\n",
        "\n",
        "Now that you've seen how to create basic Holoscan operators, let's practice by building a custom operator with specific requirements."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "204b4b85-5267-4e2d-8176-357320d4c28d",
      "metadata": {
        "id": "204b4b85-5267-4e2d-8176-357320d4c28d"
      },
      "source": [
        "## Optional Exercise 1: Create a Multi-Port Operator\n",
        "\n",
        "In this exercise, you will build a custom operator named `MyOp` with the following specifications:\n",
        "\n",
        "- 3 input ports: `in1`, `in2`, and `in3`\n",
        "- 2 output ports: `out1` and `out2`\n",
        "- The operator should always emit the data received on `in1` with the `out1` port\n",
        "- It should emit `in2` with `out2` if the operator tick (using `self.index`) is odd, and `in3` with `out2` if the tick is even\n",
        "\n",
        "Here's the template code with the parts you need to fix: -->\n",
        "\n",
        "Try implementing this operator on your own before checking the solution that is shown below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c612d51-4e9a-43dc-85d7-40aebf591b9c",
      "metadata": {
        "id": "0c612d51-4e9a-43dc-85d7-40aebf591b9c"
      },
      "outputs": [],
      "source": [
        "class <<<FIX ME>>>(<<<FIX ME>>>):\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        self.index = 0\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def setup(self, spec: OperatorSpec):\n",
        "        spec.input(<<<FIX ME>>>)\n",
        "        spec.input(<<<FIX ME>>>)\n",
        "        spec.input(<<<FIX ME>>>)\n",
        "        spec.output(<<<FIX ME>>>)\n",
        "        spec.output(<<<FIX ME>>>)\n",
        "\n",
        "    def compute(self, op_input, op_output, context):\n",
        "\n",
        "        # Always emit in1 value in out1 port\n",
        "        in1_value = op_input.receive(<<<FIX ME>>>)\n",
        "        op_output.emit(in1_value, <<<FIX ME>>>)\n",
        "\n",
        "        # each input needs to be received, regardless of utilization\n",
        "        # Even if in2 and in3 won't be utilized at the same time,\n",
        "        # they still both need to be be received\n",
        "        in2_value = op_input.receive(<<<FIX ME>>>)\n",
        "        in3_value = op_input.receive(<<<FIX ME>>>)\n",
        "\n",
        "        # If tick is even\n",
        "        if <<<FIX ME>>>:\n",
        "            op_output.emit(in2_value, \"out2\")\n",
        "        else:\n",
        "            op_output.emit(in3_value, \"out2\")\n",
        "\n",
        "        self.index += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "914d8bbd-861c-4e5b-a418-3b23a4391fd7",
      "metadata": {
        "id": "914d8bbd-861c-4e5b-a418-3b23a4391fd7"
      },
      "source": [
        "## Solution for Optional Exercise #1\n",
        "\n",
        "Here is the completed solution:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e85cc36-61bf-4b26-8de7-42f21fe26b3a",
      "metadata": {
        "id": "4e85cc36-61bf-4b26-8de7-42f21fe26b3a"
      },
      "outputs": [],
      "source": [
        "class MyOp(Operator):\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        self.index = 0\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def setup(self, spec: OperatorSpec):\n",
        "        spec.input(\"in1\")\n",
        "        spec.input(\"in2\")\n",
        "        spec.input(\"in3\")\n",
        "        spec.output(\"out1\")\n",
        "        spec.output(\"out2\")\n",
        "\n",
        "    def compute(self, op_input, op_output, context):\n",
        "\n",
        "        # Always emit in1 value in out1 port\n",
        "        in1_value = op_input.receive(\"in1\")\n",
        "        op_output.emit(in1_value, \"out1\")\n",
        "\n",
        "        # each input needs to be received, regardless of utilization\n",
        "        # Even if in2 and in3 won't be utilized at the same time,\n",
        "        # they still both need to be be received\n",
        "        in2_value = op_input.receive(\"in2\")\n",
        "        in3_value = op_input.receive(\"in3\")\n",
        "\n",
        "        # If tick is even\n",
        "        if self.index % 2 == 0:\n",
        "            op_output.emit(in2_value, \"out2\")\n",
        "        else:\n",
        "            op_output.emit(in3_value, \"out2\")\n",
        "\n",
        "        self.index += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6168c1ad-0ebc-40da-a352-132978dbbf57",
      "metadata": {
        "id": "6168c1ad-0ebc-40da-a352-132978dbbf57"
      },
      "source": [
        "## Optional Exercise 2: Create a Power-of-Two Operator\n",
        "\n",
        "For our second exercise, create a new operator called `Pow2on2` that squares the incoming integers if the clock tick is even. Otherwise, pass through the data unchanged.\n",
        "\n",
        "Here is a starting point:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "646b6efc-695b-430e-8306-f8bcf255b501",
      "metadata": {
        "id": "646b6efc-695b-430e-8306-f8bcf255b501"
      },
      "outputs": [],
      "source": [
        "class Pow2on2(Operator):\n",
        "    \"\"\"Operator that squares incoming values on even ticks.\n",
        "    This operator has:\n",
        "        inputs:  \"in1\", \"in2\"\n",
        "        outputs: \"out1\", \"out2\"\n",
        "    On even ticks, the input values are squared.\n",
        "    On odd ticks, the input values are passed through unchanged.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        # Your initialization code here\n",
        "        pass\n",
        "\n",
        "    def setup(self, spec: OperatorSpec):\n",
        "        # Define inputs and outputs\n",
        "        pass\n",
        "\n",
        "    def compute(self, op_input, op_output, context):\n",
        "        # Implement the compute logic\n",
        "        pass\n",
        "\n",
        "print(\"Try implementing this operator and integrating it into the `MyPingApp` application by replacing `PingMiddleOp` with your new `Pow2on2` operator.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8da6154-8c02-43f7-853b-071f3e809321",
      "metadata": {
        "id": "f8da6154-8c02-43f7-853b-071f3e809321"
      },
      "source": [
        "## Solution for Optional Exercise #2\n",
        "\n",
        "Here is the completed solution:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bac7548-1126-4406-900a-79848da5860e",
      "metadata": {
        "id": "9bac7548-1126-4406-900a-79848da5860e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from holoscan.conditions import CountCondition\n",
        "from holoscan.core import Application, Operator, OperatorSpec\n",
        "\n",
        "# define a custom class to represent data used in the app\n",
        "\n",
        "class ValueData:\n",
        "    \"\"\"Example of a custom Python class\"\"\"\n",
        "\n",
        "    def __init__(self, value):\n",
        "        self.data = value\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"ValueData({self.data})\"\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        return self.data == other.data\n",
        "\n",
        "    def __hash__(self):\n",
        "        return hash(self.data)\n",
        "\n",
        "# define custom Operators for use in the demo\n",
        "\n",
        "class PingTxOp(Operator):\n",
        "    \"\"\"Simple transmitter operator.\n",
        "    This operator has:\n",
        "        outputs: \"out1\", \"out2\"\n",
        "    On each tick, it transmits a `ValueData` object at each port. The\n",
        "    transmitted values are even on port1 and odd on port2 and increment with\n",
        "    each call to compute.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        self.index = 0\n",
        "        # Need to call the base class constructor last\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def setup(self, spec: OperatorSpec):\n",
        "        spec.output(\"out1\")\n",
        "        spec.output(\"out2\")\n",
        "\n",
        "    def compute(self, op_input, op_output, context):\n",
        "        value1 = ValueData(self.index)\n",
        "        self.index += 1\n",
        "        op_output.emit(value1, \"out1\")\n",
        "\n",
        "        value2 = ValueData(self.index)\n",
        "        self.index += 1\n",
        "        op_output.emit(value2, \"out2\")\n",
        "\n",
        "class PingMiddleOp(Operator):\n",
        "    \"\"\"Example of an operator modifying data.\n",
        "    This operator has:\n",
        "        inputs:  \"in1\", \"in2\"\n",
        "        outputs: \"out1\", \"out2\"\n",
        "    The data from each input is multiplied by a user-defined value.\n",
        "    In this demo, the `multiplier` parameter value is read from a \"ping.yaml\"\n",
        "    configuration file (near the bottom of this script), overriding the default\n",
        "    defined in the setup() method below.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        # If `self.multiplier` is set here (e.g., `self.multiplier = 4`), then\n",
        "        # the default value by `param()` in `setup()` will be ignored.\n",
        "        # (you can just call `spec.param(\"multiplier\")` in `setup()` to use the\n",
        "        # default value)\n",
        "        #\n",
        "        # self.multiplier = 4\n",
        "        self.count = 1\n",
        "\n",
        "        # Need to call the base class constructor last\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def setup(self, spec: OperatorSpec):\n",
        "        spec.input(\"in1\")\n",
        "        spec.input(\"in2\")\n",
        "        spec.output(\"out1\")\n",
        "        spec.output(\"out2\")\n",
        "        spec.param(\"multiplier\", 2)\n",
        "\n",
        "    def compute(self, op_input, op_output, context):\n",
        "        value1 = op_input.receive(\"in1\")\n",
        "        value2 = op_input.receive(\"in2\")\n",
        "        print(f\"Middle message received (count: {self.count})\")\n",
        "        self.count += 1\n",
        "\n",
        "        print(f\"Middle message value1: {value1.data}\")\n",
        "        print(f\"Middle message value2: {value2.data}\")\n",
        "\n",
        "        # Multiply the values by the multiplier parameter\n",
        "        value1.data *= self.multiplier\n",
        "        value2.data *= self.multiplier\n",
        "\n",
        "        op_output.emit(value1, \"out1\")\n",
        "        op_output.emit(value2, \"out2\")\n",
        "\n",
        "class Pow2On2Op(Operator):\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        # If `self.multiplier` is set here (e.g., `self.multiplier = 4`), then\n",
        "        # the default value by `param()` in `setup()` will be ignored.\n",
        "        # (you can just call `spec.param(\"multiplier\")` in `setup()` to use the\n",
        "        # default value)\n",
        "        #\n",
        "        # self.multiplier = 4\n",
        "        self.count = 0\n",
        "\n",
        "        # Need to call the base class constructor last\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def setup(self, spec: OperatorSpec):\n",
        "        spec.input(\"in1\")\n",
        "        spec.input(\"in2\")\n",
        "        spec.output(\"out1\")\n",
        "        spec.output(\"out2\")\n",
        "        spec.param(\"multiplier\", 2)\n",
        "\n",
        "    def compute(self, op_input, op_output, context):\n",
        "        value1 = op_input.receive(\"in1\")\n",
        "        value2 = op_input.receive(\"in2\")\n",
        "\n",
        "        print(f\"Middle message received (count: {self.count})\")\n",
        "        print(f\"Middle message value1: {value1.data}\")\n",
        "        print(f\"Middle message value2: {value2.data}\")\n",
        "\n",
        "        # If count is even\n",
        "        if self.count % 2 == 0:\n",
        "            value1.data *= value1.data\n",
        "            value2.data *= value2.data\n",
        "\n",
        "        # Increment count\n",
        "        self.count += 1\n",
        "\n",
        "        op_output.emit(value1, \"out1\")\n",
        "        op_output.emit(value2, \"out2\")\n",
        "\n",
        "class PingRxOp(Operator):\n",
        "    \"\"\"Simple receiver operator.\n",
        "    This operator has:\n",
        "        input: \"receivers\"\n",
        "    This is an example of a native operator that can dynamically have any\n",
        "    number of inputs connected to is \"receivers\" port.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        # Need to call the base class constructor last\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def setup(self, spec: OperatorSpec):\n",
        "        spec.param(\"receivers\", kind=\"receivers\")\n",
        "\n",
        "    def compute(self, op_input, op_output, context):\n",
        "        values = op_input.receive(\"receivers\")\n",
        "        print(f\"Rx message value1: {values[0].data}\")\n",
        "        print(f\"Rx message value2: {values[1].data}\")\n",
        "\n",
        "class MyPingPowApp(Application):\n",
        "    def compose(self):\n",
        "        # Configure the operators. Here we use CountCondition to terminate\n",
        "        # execution after a specific number of messages have been sent.\n",
        "        tx = PingTxOp(self, CountCondition(self, 10), name=\"tx\")\n",
        "        mx = Pow2On2Op(self, name=\"mx\")\n",
        "        rx = PingRxOp(self, name=\"rx\")\n",
        "\n",
        "        # Connect the operators into the workflow:  tx -> mx -> rx\n",
        "        self.add_flow(tx, mx, {(\"out1\", \"in1\"), (\"out2\", \"in2\")})\n",
        "        self.add_flow(mx, rx, {(\"out1\", \"receivers\"), (\"out2\", \"receivers\")})\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app = MyPingPowApp()\n",
        "    # no config file\n",
        "    app.config(\"\")\n",
        "    app.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df984702-9270-4cab-8f4c-d1976819bf35",
      "metadata": {
        "id": "df984702-9270-4cab-8f4c-d1976819bf35"
      },
      "source": [
        "## Key Learning Points\n",
        "\n",
        "These exercises help you practice important Holoscan concepts:\n",
        "\n",
        "1. **Port Configuration**: Setting up input and output ports\n",
        "2. **Conditional Processing**: Implementing different logic based on conditions\n",
        "3. **Operator State**: Maintaining state across multiple compute calls\n",
        "4. **Data Flow**: Understanding how data moves between operators\n",
        "\n",
        "In the next section, we'll explore pre-built Holoscan operators and how to use them in more complex applications."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ba67fe1-bc5e-4baa-b216-5d21985c7ced",
      "metadata": {
        "id": "4ba67fe1-bc5e-4baa-b216-5d21985c7ced"
      },
      "source": [
        "# Using Pre-Built Holoscan Operators\n",
        "\n",
        "While creating custom operators is powerful, Holoscan also provides a rich set of pre-built operators for common tasks. These operators are optimized for performance and can significantly reduce development time."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d49584b-e381-43b4-a01c-0e785da74606",
      "metadata": {
        "id": "5d49584b-e381-43b4-a01c-0e785da74606"
      },
      "source": [
        "## Core Holoscan Operators\n",
        "\n",
        "The Holoscan SDK includes several pre-built operators for common tasks:\n",
        "\n",
        "| Operator | Description |\n",
        "|----------|-------------|\n",
        "| **VideoStreamReplayerOp** | Outputs video frames as Holoscan Tensor objects |\n",
        "| **FormatConverterOp** | Converts data formats and performs operations like resizing |\n",
        "| **InferenceOp** | Executes AI inference on one or multiple models |\n",
        "| **HolovizOp** | High-speed viewer for visualization of images and overlay data |\n",
        "| **AJA Source** | Enables GPU-Direct RDMA with AJA capture cards |\n",
        "\n",
        "These operators are highly optimized and designed to work together in GPU-accelerated pipelines."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7044b9a-8e68-44ca-9cfe-582e8e7dffcb",
      "metadata": {
        "id": "b7044b9a-8e68-44ca-9cfe-582e8e7dffcb"
      },
      "source": [
        "## Benefits of Pre-Built Operators\n",
        "\n",
        "Using pre-built operators offers several advantages:\n",
        "\n",
        "1. **Performance**: Optimized for maximum throughput and minimum latency\n",
        "2. **Reliability**: Thoroughly tested and production-ready\n",
        "3. **Interoperability**: Designed to work seamlessly with other Holoscan components\n",
        "4. **Configurability**: Easily customizable through YAML configuration files\n",
        "5. **GPU Acceleration**: Built to maximize GPU utilization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76adfe61-cc40-4400-bbed-3060de01a6aa",
      "metadata": {
        "id": "76adfe61-cc40-4400-bbed-3060de01a6aa"
      },
      "source": [
        "## Configuring Operators with YAML\n",
        "\n",
        "Most pre-built operators can be configured using YAML files, which provide a clean separation between code and configuration:\n",
        "\n",
        "```yaml\n",
        "# Example operator configuration in YAML\n",
        "replayer_source:\n",
        "  directory: ./data\n",
        "  batch_size: 1\n",
        "  loop: false\n",
        "  realtime: false\n",
        "\n",
        "preprocessor:\n",
        "  resize_width: 960\n",
        "  resize_height: 544\n",
        "  scale_min: 0\n",
        "  scale_max: 255\n",
        "\n",
        "inference:\n",
        "  backend: \"trt\"\n",
        "  model_path_map:\n",
        "    face_detect: /path/to/model.onnx\n",
        "```\n",
        "\n",
        "This approach makes it easy to adjust parameters without changing code, and allows for different configurations in different environments."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f98296b-2195-4649-8447-22832f30d866",
      "metadata": {
        "id": "0f98296b-2195-4649-8447-22832f30d866"
      },
      "source": [
        "## Example: Using Format Converter\n",
        "\n",
        "Here's a simple example of using the `FormatConverterOp` in a pipeline (**DON'T RUN - Cell is for illustration purposes only**):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "495cb29d-96b8-47dc-ab2a-212e77532e1d",
      "metadata": {
        "id": "495cb29d-96b8-47dc-ab2a-212e77532e1d"
      },
      "outputs": [],
      "source": [
        "from holoscan.operators import FormatConverterOp\n",
        "\n",
        "# In your Application's compose method:\n",
        "format_converter = FormatConverterOp(\n",
        "    self,\n",
        "    name=\"preprocessor\",\n",
        "    pool=pool,\n",
        "    resize_width=960,\n",
        "    resize_height=544,\n",
        "    scale_min=0,\n",
        "    scale_max=255\n",
        ")\n",
        "\n",
        "# Connect it to other operators\n",
        "self.add_flow(source, format_converter)\n",
        "self.add_flow(format_converter, next_op)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67419342-313f-40ca-aad8-667d9398c0b3",
      "metadata": {
        "id": "67419342-313f-40ca-aad8-667d9398c0b3"
      },
      "source": [
        "The `FormatConverterOp` takes input images, resizes them to the specified dimensions, and scales pixel values to the specified range."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d8fe7d1-b6ac-46d6-b562-862b5b7f8092",
      "metadata": {
        "id": "5d8fe7d1-b6ac-46d6-b562-862b5b7f8092"
      },
      "source": [
        "## Example: Using Inference Operator\n",
        "\n",
        "The `InferenceOp` is used to run neural network inference (**DON'T RUN - Cell is for illustration purposes only**):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97527e81-9c15-4a25-aa29-e479cba1a109",
      "metadata": {
        "id": "97527e81-9c15-4a25-aa29-e479cba1a109"
      },
      "outputs": [],
      "source": [
        "from holoscan.operators import InferenceOp\n",
        "\n",
        "# In your Application's compose method:\n",
        "inference = InferenceOp(\n",
        "    self,\n",
        "    name=\"inference\",\n",
        "    allocator=pool,\n",
        "    backend=\"trt\",  # TensorRT backend\n",
        "    model_path_map={\n",
        "        \"face_detect\": os.path.join(self.data_path, \"model.onnx\")\n",
        "    }\n",
        ")\n",
        "\n",
        "# Connect it to other operators\n",
        "self.add_flow(preprocessor, inference, {(\"\", \"receivers\")})\n",
        "self.add_flow(inference, postprocessor, {(\"transmitter\", \"in\")})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43c43f71-7185-4e4d-ad89-db1f47e17dc9",
      "metadata": {
        "id": "43c43f71-7185-4e4d-ad89-db1f47e17dc9"
      },
      "source": [
        "The `InferenceOp` takes preprocessed input data, runs inference using the specified model, and outputs the results to the next operator in the pipeline.\n",
        "\n",
        "In the next section, we'll build a complete application using these pre-built operators to perform real-time face detection on a video stream."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1ad65d7-b940-4710-b43f-8b6295d8c286",
      "metadata": {
        "id": "d1ad65d7-b940-4710-b43f-8b6295d8c286"
      },
      "source": [
        "# TAO PeopleNet Detection: Building a Complete Application\n",
        "\n",
        "In this section, we'll build a complete Holoscan application for face detection using NVIDIA's PeopleNet model from the Transfer Learning Toolkit (TLT) and Training, Adaptation, and Optimization (TAO) platform."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown, Image, display\n",
        "\n",
        "# Display descriptive markdown\n",
        "display(Markdown(\"\"\"\n",
        "# Application Overview\n",
        "\n",
        "Our application will:\n",
        "\n",
        "1. Load video frames from a source\n",
        "2. Preprocess the frames for inference\n",
        "3. Detect faces using the PeopleNet model\n",
        "4. Postprocess the detection results\n",
        "5. Visualize the results with bounding boxes\n",
        "6. Save the processed video\n",
        "\n",
        "Below is a diagram of our pipeline:\n",
        "\"\"\"))\n",
        "\n",
        "# Show the face and people detection pipeline image\n",
        "display(Image(filename='/content/images/face_and_people_detection_app.png', width=1200))"
      ],
      "metadata": {
        "id": "rgDj6WhMkCop"
      },
      "id": "rgDj6WhMkCop",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "78f293d3-ffe8-449d-bf06-5f9aa02d2afb",
      "metadata": {
        "id": "78f293d3-ffe8-449d-bf06-5f9aa02d2afb"
      },
      "source": [
        "## Required Imports\n",
        "\n",
        "We will start by importing the necessary modules:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Downgrade NumPy (critical for compatibility)\n",
        "!pip install -q \"numpy==1.23.5\" --force-reinstall\n",
        "\n",
        "# Step 2: Check if NumPy downgrade worked\n",
        "import numpy as np\n",
        "print(f\"NumPy version after downgrade: {np.__version__}\")\n",
        "\n",
        "# Step 3: Set up a lightweight CUDA bridge directory structure\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Create a directory for our custom CUDA bridge\n",
        "!mkdir -p /tmp/cuda_bridge/lib64\n",
        "\n",
        "# Find existing libcuda.so.1 files\n",
        "!find / -name \"libcuda.so.1\" 2>/dev/null\n",
        "\n",
        "# Create symbolic links to any existing CUDA libraries\n",
        "!ln -sf /usr/local/cuda-12.5/compat/libcuda.so.1 /tmp/cuda_bridge/lib64/libcuda.so.1\n",
        "\n",
        "# Step 4: Update environment variables to use our bridge\n",
        "os.environ[\"LD_LIBRARY_PATH\"] = \"/tmp/cuda_bridge/lib64:\" + os.environ.get(\"LD_LIBRARY_PATH\", \"\")\n",
        "os.environ[\"CUDA_PATH\"] = \"/usr/local/cuda-12.5\"\n",
        "\n",
        "# Step 5: Create a minimal CuPy mock\n",
        "# This approach creates a lightweight CuPy substitute that doesn't need GPU drivers\n",
        "if \"cupy\" not in sys.modules:\n",
        "    print(\"Creating CuPy mock...\")\n",
        "\n",
        "    # Create a mock CuPy module\n",
        "    from types import ModuleType\n",
        "    cupy_mock = ModuleType(\"cupy\")\n",
        "    sys.modules[\"cupy\"] = cupy_mock\n",
        "\n",
        "    # Setup basic NumPy compatibility\n",
        "    cupy_mock.ndarray = np.ndarray\n",
        "    cupy_mock.array = lambda arr, dtype=None: np.array(arr, dtype=dtype)\n",
        "    cupy_mock.asarray = lambda arr, dtype=None: np.asarray(arr, dtype=dtype)\n",
        "    cupy_mock.zeros = lambda *args, **kwargs: np.zeros(*args, **kwargs)\n",
        "    cupy_mock.ones = lambda *args, **kwargs: np.ones(*args, **kwargs)\n",
        "\n",
        "    # Add math operations\n",
        "    cupy_mock.add = lambda x, y: np.add(x, y)\n",
        "    cupy_mock.subtract = lambda x, y: np.subtract(x, y)\n",
        "    cupy_mock.multiply = lambda x, y: np.multiply(x, y)\n",
        "    cupy_mock.divide = lambda x, y: np.divide(x, y)\n",
        "\n",
        "    # Create CUDA mock submodule\n",
        "    cuda_mock = ModuleType(\"cuda\")\n",
        "    cupy_mock.cuda = cuda_mock\n",
        "    cuda_mock.is_available = lambda: False\n",
        "    cuda_mock.runtime = ModuleType(\"runtime\")\n",
        "\n",
        "    # Create device mock\n",
        "    cuda_mock.Device = type(\"Device\", (), {\n",
        "        \"__call__\": lambda self, device_id: type(\"DeviceObject\", (), {\n",
        "            \"name\": lambda: \"CPU (Emulated)\",\n",
        "            \"id\": lambda: device_id,\n",
        "            \"attributes\": {}\n",
        "        })(),\n",
        "        \"count\": lambda: 1\n",
        "    })()\n",
        "\n",
        "    # Add array transfer functions\n",
        "    cupy_mock.get = lambda arr: arr\n",
        "    cupy_mock.to_gpu = lambda arr: arr\n",
        "\n",
        "    # Add common attributes\n",
        "    setattr(cupy_mock, \"__version__\", \"12.2.0-mock\")\n",
        "    setattr(cupy_mock, \"float32\", np.float32)\n",
        "    setattr(cupy_mock, \"float64\", np.float64)\n",
        "    setattr(cupy_mock, \"float_\", np.float64)\n",
        "\n",
        "    print(\"CuPy mock created successfully\")\n",
        "else:\n",
        "    print(\"CuPy already exists in sys.modules\")\n",
        "\n",
        "# Step 6: Install Holoscan again (with our mock cupy already in place)\n",
        "!pip install -q holoscan\n",
        "\n",
        "# Step 7: Test if our approach works\n",
        "try:\n",
        "    import cupy as cp\n",
        "    import holoscan\n",
        "\n",
        "    print(f\"CuPy version: {cp.__version__}\")\n",
        "    print(f\"Holoscan version: {holoscan.__version__}\")\n",
        "\n",
        "    # Try creating arrays with our mock\n",
        "    a = cp.array([1, 2, 3])\n",
        "    b = cp.array([4, 5, 6])\n",
        "    c = cp.add(a, b)\n",
        "    print(f\"Array test: {a} + {b} = {c}\")\n",
        "\n",
        "    print(\"CuPy mock is working correctly with NumPy backend\")\n",
        "except Exception as e:\n",
        "    print(f\"Error testing CuPy mock: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "id": "eakvIxBCCg-V"
      },
      "id": "eakvIxBCCg-V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Search for NVIDIA libraries\n",
        "print(\"Searching for NVIDIA driver libraries...\\n\")\n",
        "\n",
        "# Look for libnvidia-ml.so specifically\n",
        "print(\"=== Looking for libnvidia-ml.so ===\")\n",
        "!find / -name \"libnvidia-ml.so*\" 2>/dev/null\n",
        "\n",
        "# Look for other key NVIDIA libraries\n",
        "print(\"\\n=== Looking for other NVIDIA libraries ===\")\n",
        "!find / -name \"libcuda.so*\" 2>/dev/null\n",
        "!find / -name \"libnvidia-*.so*\" 2>/dev/null | head -20\n",
        "\n",
        "# Check CUDA runtime location\n",
        "print(\"\\n=== CUDA Runtime Location ===\")\n",
        "!find /usr/local/cuda* -name \"libcudart.so*\" 2>/dev/null\n",
        "\n",
        "# Check if we can access NVIDIA driver version\n",
        "print(\"\\n=== Trying to access driver version ===\")\n",
        "!cat /proc/driver/nvidia/version 2>/dev/null || echo \"No NVIDIA driver version file found\"\n",
        "\n",
        "# Check loaded kernel modules\n",
        "print(\"\\n=== Checking for NVIDIA kernel modules ===\")\n",
        "!lsmod | grep -i nvidia\n",
        "\n",
        "# Check if CUDA driver is loaded in LD_LIBRARY_PATH\n",
        "print(\"\\n=== Checking LD_LIBRARY_PATH ===\")\n",
        "import os\n",
        "print(os.environ.get(\"LD_LIBRARY_PATH\", \"Not set\"))\n",
        "\n",
        "# Check if we can create a minimal CUDA context\n",
        "print(\"\\n=== Testing minimal CUDA context creation ===\")\n",
        "!python -c \"import ctypes; cuda = ctypes.CDLL('libcuda.so.1', mode=ctypes.RTLD_GLOBAL); print('CUDA library loaded via ctypes')\" 2>&1 || echo \"Failed to load CUDA library\""
      ],
      "metadata": {
        "id": "9kunLPvhD48s"
      },
      "id": "9kunLPvhD48s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2db7a721-740e-49e8-8a87-b1aa8cf223a1",
      "metadata": {
        "id": "2db7a721-740e-49e8-8a87-b1aa8cf223a1"
      },
      "outputs": [],
      "source": [
        "# import cupy as cp\n",
        "# import holoscan as hs\n",
        "# import numpy as np\n",
        "# import os\n",
        "# import cv2\n",
        "# import glob\n",
        "# import time\n",
        "# import logging\n",
        "# import threading\n",
        "# import subprocess\n",
        "# from holoscan.core import Application, Operator, OperatorSpec\n",
        "# from holoscan.gxf import Entity\n",
        "# from holoscan.operators import (\n",
        "#     VideoStreamReplayerOp,\n",
        "#     FormatConverterOp,\n",
        "#     InferenceOp,\n",
        "#     HolovizOp,\n",
        "# )\n",
        "# from holoscan.resources import UnboundedAllocator\n",
        "# from holoscan.schedulers import GreedyScheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edd3ae9b-d21d-4765-b2e8-842217db910d",
      "metadata": {
        "id": "edd3ae9b-d21d-4765-b2e8-842217db910d"
      },
      "source": [
        "## Custom Operators for the Pipeline\n",
        "\n",
        "We will create two custom operators to process the model's input and output."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cdabc6b-a885-42c8-a05c-7edce38a41ea",
      "metadata": {
        "id": "1cdabc6b-a885-42c8-a05c-7edce38a41ea"
      },
      "source": [
        "### Preprocessor Operator\n",
        "\n",
        "This operator prepares the input for inference:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe4bcbf6-4406-4940-b583-35b105e5403e",
      "metadata": {
        "id": "fe4bcbf6-4406-4940-b583-35b105e5403e"
      },
      "outputs": [],
      "source": [
        "class PreprocessorOp(Operator):\n",
        "    \"\"\"Operator to format input image for inference\"\"\"\n",
        "    def setup(self, spec: OperatorSpec):\n",
        "        spec.input(\"in\")\n",
        "        spec.output(\"out\")\n",
        "\n",
        "    def compute(self, op_input, op_output, context):\n",
        "        # Get input message\n",
        "        in_message = op_input.receive(\"in\")\n",
        "\n",
        "        # Transpose\n",
        "        tensor = cp.asarray(in_message.get(\"preprocessed\")).get()\n",
        "        # OBS: Numpy conversion and moveaxis is needed to avoid strange\n",
        "        # strides issue when doing inference\n",
        "        tensor = np.moveaxis(tensor, 2, 0)[None]\n",
        "        tensor = cp.asarray(tensor)\n",
        "\n",
        "        # Create output message\n",
        "        out_message = {\"preprocessed\": tensor}\n",
        "        op_output.emit(out_message, \"out\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd6dcd02-d41f-43d6-b3be-9e6cef8d68e2",
      "metadata": {
        "id": "dd6dcd02-d41f-43d6-b3be-9e6cef8d68e2"
      },
      "source": [
        "### Postprocessor Operator\n",
        "\n",
        "This operator processes the model's output to extract face detections:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e536892f-e524-4b24-a9c8-936b6a4bc128",
      "metadata": {
        "id": "e536892f-e524-4b24-a9c8-936b6a4bc128"
      },
      "outputs": [],
      "source": [
        "class PostprocessorOp(Operator):\n",
        "    \"\"\"Operator to post-process inference output:\n",
        "    * Reparameterize bounding boxes\n",
        "    * Non-max suppression\n",
        "    * Make boxes compatible with Holoviz\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def setup(self, spec: OperatorSpec):\n",
        "        spec.input(\"in\")\n",
        "        spec.output(\"out\")\n",
        "        spec.param(\"iou_threshold\", 0.15)\n",
        "        spec.param(\"score_threshold\", 0.3)  # Lower threshold to catch more potential faces\n",
        "        spec.param(\"image_width\", None)\n",
        "        spec.param(\"image_height\", None)\n",
        "        spec.param(\"box_scale\", None)\n",
        "        spec.param(\"box_offset\", None)\n",
        "        spec.param(\"grid_height\", None)\n",
        "        spec.param(\"grid_width\", None)\n",
        "\n",
        "    def compute(self, op_input, op_output, context):\n",
        "        # Get input message\n",
        "        in_message = op_input.receive(\"in\")\n",
        "\n",
        "        # Convert input to cupy array\n",
        "        boxes = cp.asarray(in_message.get(\"boxes\"))[0, ...]\n",
        "        scores = cp.asarray(in_message.get(\"scores\"))[0, ...]\n",
        "\n",
        "        # PeopleNet has three classes:\n",
        "        # 0. Person\n",
        "        # 1. Bag\n",
        "        # 2. Face\n",
        "        # Here we only keep the Face class (index 2)\n",
        "        face_boxes = boxes[[8, 9, 10, 11], ...][None]  # Indices 8-11 correspond to face boxes\n",
        "        face_scores = scores[[2], ...][None]  # Index 2 corresponds to face scores\n",
        "\n",
        "        # Create output dictionary with just faces\n",
        "        out = {\"faces\": None}\n",
        "\n",
        "        # Reparameterize face boxes\n",
        "        out[\"faces\"], scores_nms = self.reparameterize_boxes(\n",
        "            face_boxes,\n",
        "            face_scores\n",
        "        )\n",
        "\n",
        "        # Non-max suppression\n",
        "        out[\"faces\"], _ = self.nms(out[\"faces\"], scores_nms)\n",
        "\n",
        "        # Reshape for HoloViz\n",
        "        if len(out[\"faces\"]) == 0:\n",
        "            out[\"faces\"] = np.zeros([1, 2, 2]).astype(np.float32)\n",
        "        else:\n",
        "            out[\"faces\"][:, [0, 2]] /= self.image_width\n",
        "            out[\"faces\"][:, [1, 3]] /= self.image_height\n",
        "            out[\"faces\"] = cp.reshape(out[\"faces\"][None], (1, -1, 2))\n",
        "\n",
        "        # Create output message\n",
        "        op_output.emit(out, \"out\")\n",
        "\n",
        "    def nms(self, boxes, scores):\n",
        "        \"\"\"Non-max suppression (NMS)\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        boxes : array (4, n)\n",
        "        scores : array (n,)\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        boxes : array (m, 4)\n",
        "        scores : array (m,)\n",
        "\n",
        "        \"\"\"\n",
        "        if len(boxes) == 0:\n",
        "            return cp.asarray([]), cp.asarray([])\n",
        "\n",
        "        # Get coordinates\n",
        "        x0, y0, x1, y1 = boxes[0, :], boxes[1, :], boxes[2, :], boxes[3, :]\n",
        "\n",
        "        # Area of bounding boxes\n",
        "        area = (x1 - x0 + 1) * (y1 - y0 + 1)\n",
        "\n",
        "        # Get indices of sorted scores\n",
        "        indices = cp.argsort(scores)\n",
        "\n",
        "        # Output boxes and scores\n",
        "        boxes_out, scores_out = [], []\n",
        "\n",
        "        # Iterate over bounding boxes\n",
        "        while len(indices) > 0:\n",
        "            # Get index with highest score from remaining indices\n",
        "            index = indices[-1]\n",
        "\n",
        "            # Pick bounding box with highest score\n",
        "            boxes_out.append(boxes[:, index])\n",
        "            scores_out.append(scores[index])\n",
        "\n",
        "            # Get coordinates\n",
        "            x00 = cp.maximum(x0[index], x0[indices[:-1]])\n",
        "            x11 = cp.minimum(x1[index], x1[indices[:-1]])\n",
        "            y00 = cp.maximum(y0[index], y0[indices[:-1]])\n",
        "            y11 = cp.minimum(y1[index], y1[indices[:-1]])\n",
        "\n",
        "            # Compute IOU\n",
        "            width = cp.maximum(0, x11 - x00 + 1)\n",
        "            height = cp.maximum(0, y11 - y00 + 1)\n",
        "            overlap = width * height\n",
        "            union = area[index] + area[indices[:-1]] - overlap\n",
        "            iou = overlap / union\n",
        "\n",
        "            # Threshold and prune\n",
        "            left = cp.where(iou < self.iou_threshold)\n",
        "            indices = indices[left]\n",
        "\n",
        "        # To array\n",
        "        boxes = cp.asarray(boxes_out)\n",
        "        scores = cp.asarray(scores_out)\n",
        "\n",
        "        return boxes, scores\n",
        "\n",
        "    def reparameterize_boxes(self, boxes, scores):\n",
        "        \"\"\"Reparameterize boxes from corner+width+height to corner+corner.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        boxes : array (1, 4, grid_height, grid_width)\n",
        "        scores : array (1, 1, grid_height, grid_width)\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        boxes : array (4, n)\n",
        "        scores : array (n,)\n",
        "\n",
        "        \"\"\"\n",
        "        cell_height = self.image_height / self.grid_height\n",
        "        cell_width = self.image_width / self.grid_width\n",
        "\n",
        "        # Generate the grid coordinates\n",
        "        mx, my = cp.meshgrid(cp.arange(self.grid_width), cp.arange(self.grid_height))\n",
        "        mx = mx.astype(np.float32).reshape((1, 1, self.grid_height, self.grid_width))\n",
        "        my = my.astype(np.float32).reshape((1, 1, self.grid_height, self.grid_width))\n",
        "\n",
        "        # Compute the box corners\n",
        "        xmin = -(boxes[0, 0, ...] + self.box_offset) * self.box_scale + mx * cell_width\n",
        "        ymin = -(boxes[0, 1, ...] + self.box_offset) * self.box_scale + my * cell_height\n",
        "        xmax = (boxes[0, 2, ...] + self.box_offset) * self.box_scale + mx * cell_width\n",
        "        ymax = (boxes[0, 3, ...] + self.box_offset) * self.box_scale + my * cell_height\n",
        "        boxes = cp.concatenate([xmin, ymin, xmax, ymax], axis=1)\n",
        "\n",
        "        # Select the scores that are above the threshold\n",
        "        scores_mask = scores > self.score_threshold\n",
        "        scores = scores[scores_mask]\n",
        "        scores_mask = cp.repeat(scores_mask, 4, axis=1)\n",
        "        boxes = boxes[scores_mask]\n",
        "\n",
        "        # Reshape after masking\n",
        "        n = int(boxes.size / 4)\n",
        "        boxes = boxes.reshape(4, n)\n",
        "\n",
        "        return boxes, scores"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8aadd691-bbdd-4b97-8c06-7eec609cdfc9",
      "metadata": {
        "id": "8aadd691-bbdd-4b97-8c06-7eec609cdfc9"
      },
      "source": [
        "### Visualization Sink Operator\n",
        "\n",
        "This operator renders the detection results and saves frames:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf0c5e29-d475-441d-9e25-f9d51c104eec",
      "metadata": {
        "id": "cf0c5e29-d475-441d-9e25-f9d51c104eec"
      },
      "outputs": [],
      "source": [
        "class VisualizationSinkOp(Operator):\n",
        "    def __init__(self, fragment, name=\"visualization_sink\",\n",
        "                 save_frames=False,\n",
        "                 output_dir=\"detection_results\",\n",
        "                 output_video=\"./scripts/tao_peoplenet/data/people_faces_detected.webm\",\n",
        "                 *args, **kwargs):\n",
        "        import os\n",
        "        import time\n",
        "        import cv2\n",
        "        import subprocess\n",
        "        import threading\n",
        "\n",
        "        # Initialize properties\n",
        "        self.frame_count = 0\n",
        "        self.save_frames = save_frames\n",
        "        self.output_dir = output_dir\n",
        "\n",
        "        # Set up directories and paths\n",
        "        self.frames_dir = os.path.join(os.path.dirname(output_video), \"temp_frames\")\n",
        "        os.makedirs(self.frames_dir, exist_ok=True)\n",
        "\n",
        "        self.output_video_base = os.path.splitext(output_video)[0]\n",
        "        self.output_video_webm = f\"{self.output_video_base}.webm\"\n",
        "\n",
        "        self.start_time = time.time()\n",
        "        self.frames_saved = 0\n",
        "        self.frame_paths = []\n",
        "\n",
        "        # Call the base class constructor\n",
        "        super().__init__(fragment, name=name, *args, **kwargs)\n",
        "\n",
        "        # Check for ffmpeg\n",
        "        try:\n",
        "            result = subprocess.run(['ffmpeg', '-version'],\n",
        "                                  stdout=subprocess.PIPE,\n",
        "                                  stderr=subprocess.PIPE,\n",
        "                                  text=True)\n",
        "            self.ffmpeg_available = result.returncode == 0\n",
        "            print(f\"ffmpeg available: {self.ffmpeg_available}\")\n",
        "        except Exception:\n",
        "            self.ffmpeg_available = False\n",
        "            print(\"ffmpeg not available\")\n",
        "\n",
        "        # Create output directory\n",
        "        output_video_dir = os.path.dirname(self.output_video_webm)\n",
        "        if output_video_dir:\n",
        "            os.makedirs(output_video_dir, exist_ok=True)\n",
        "            print(f\"Ensured output directory exists: {output_video_dir}\")\n",
        "\n",
        "    def setup(self, spec):\n",
        "        spec.input(\"in\")\n",
        "        spec.input(\"original_image\")\n",
        "\n",
        "    def compute(self, op_input, op_output, context):\n",
        "        # Import necessary modules\n",
        "        import time\n",
        "        import numpy as np\n",
        "        import cv2\n",
        "        import os\n",
        "\n",
        "        # Start timing this frame\n",
        "        frame_start_time = time.time()\n",
        "\n",
        "        # Get the detection results\n",
        "        detection_data = op_input.receive(\"in\")\n",
        "        # Get the original image\n",
        "        original_image = op_input.receive(\"original_image\")\n",
        "\n",
        "        if detection_data and original_image:\n",
        "            # Try to get original image data\n",
        "            img_array = None\n",
        "\n",
        "            if isinstance(original_image, dict) and '' in original_image:\n",
        "                img_tensor = original_image['']\n",
        "\n",
        "                # Try to access the image data using cupy\n",
        "                try:\n",
        "                    import cupy as cp\n",
        "                    if hasattr(img_tensor, '__cuda_array_interface__'):\n",
        "                        # Convert GPU tensor to cupy array and then to numpy\n",
        "                        img_array = cp.asarray(img_tensor).get()\n",
        "                except (ImportError, Exception):\n",
        "                    # Don't print error for better performance\n",
        "                    pass\n",
        "\n",
        "            # If we couldn't get the original image, create a blank one\n",
        "            if img_array is None:\n",
        "                h, w = 1080, 1920\n",
        "                img_array = np.ones((h, w, 3), dtype=np.uint8) * 80  # Medium gray\n",
        "                print(\"Created fallback image (original image access failed)\")\n",
        "\n",
        "            # Make a copy for visualization (in RGB format)\n",
        "            processed_img = img_array.copy()\n",
        "            h, w = processed_img.shape[:2]\n",
        "\n",
        "            # Process detection data to extract face boxes\n",
        "            face_boxes = []\n",
        "\n",
        "            if isinstance(detection_data, dict) and 'faces' in detection_data:\n",
        "                try:\n",
        "                    # Convert to numpy array using cupy\n",
        "                    faces_data = detection_data['faces']\n",
        "                    if faces_data is not None and hasattr(faces_data, '__cuda_array_interface__'):\n",
        "                        import cupy as cp\n",
        "                        faces_array = cp.asarray(faces_data).get()\n",
        "\n",
        "                        # Process face detection coordinates\n",
        "                        if len(faces_array.shape) == 3 and faces_array.shape[0] == 1:\n",
        "                            # Process pairs of points as bounding boxes\n",
        "                            for i in range(0, faces_array.shape[1], 2):\n",
        "                                if i+1 < faces_array.shape[1]:\n",
        "                                    # Get normalized coordinates\n",
        "                                    x1 = float(faces_array[0, i, 0])\n",
        "                                    y1 = float(faces_array[0, i, 1])\n",
        "                                    x2 = float(faces_array[0, i+1, 0])\n",
        "                                    y2 = float(faces_array[0, i+1, 1])\n",
        "\n",
        "                                    # Skip if all coordinates are very close to zero\n",
        "                                    if not np.allclose([x1, y1, x2, y2], 0, atol=1e-5):\n",
        "                                        # Convert normalized coordinates to pixel coordinates\n",
        "                                        px1, py1 = int(x1 * w), int(y1 * h)\n",
        "                                        px2, py2 = int(x2 * w), int(y2 * h)\n",
        "\n",
        "                                        # Ensure coordinates are within image bounds\n",
        "                                        px1 = max(0, min(px1, w-1))\n",
        "                                        py1 = max(0, min(py1, h-1))\n",
        "                                        px2 = max(0, min(px2, w-1))\n",
        "                                        py2 = max(0, min(py2, h-1))\n",
        "\n",
        "                                        # Ensure proper orientation (x2 > x1, y2 > y1)\n",
        "                                        if px1 > px2:\n",
        "                                            px1, px2 = px2, px1\n",
        "                                        if py1 > py2:\n",
        "                                            py1, py2 = py2, py1\n",
        "\n",
        "                                        # Add face box if it has reasonable dimensions\n",
        "                                        if (px2 - px1) > 10 and (py2 - py1) > 10:\n",
        "                                            face_boxes.append([px1, py1, px2, py2])\n",
        "                except Exception:\n",
        "                    # Don't print error for better performance\n",
        "                    pass\n",
        "\n",
        "            # Calculate timing information\n",
        "            frame_time = (time.time() - frame_start_time) * 1000  # ms\n",
        "            total_time = time.time() - self.start_time\n",
        "            fps = self.frame_count / total_time if total_time > 0 else 0\n",
        "\n",
        "            # Add frame information to the image - using RGB colors\n",
        "            font = cv2.FONT_HERSHEY_DUPLEX\n",
        "            # Yellow in RGB is (255, 255, 0)\n",
        "            cv2.putText(processed_img, f\"Frame: {self.frame_count} - Faces: {len(face_boxes)}\",\n",
        "                      (20, 40), font, 1, (255, 255, 0), 2)\n",
        "\n",
        "            # Add performance metrics\n",
        "            cv2.putText(processed_img, f\"Frame time: {frame_time:.2f}ms\",\n",
        "                      (20, 80), font, 0.6, (0, 255, 255), 1)\n",
        "            cv2.putText(processed_img, f\"Average FPS: {fps:.1f}\",\n",
        "                      (20, 110), font, 0.6, (0, 255, 255), 1)\n",
        "\n",
        "            # Draw face boxes - using RGB colors\n",
        "            for i, box in enumerate(face_boxes):\n",
        "                try:\n",
        "                    x1, y1, x2, y2 = box\n",
        "\n",
        "                    # Draw rectangle (Red in RGB is (255, 0, 0))\n",
        "                    cv2.rectangle(processed_img, (x1, y1), (x2, y2), (255, 0, 0), 3)\n",
        "\n",
        "                    # Add label (Red in RGB is (255, 0, 0))\n",
        "                    cv2.putText(processed_img, f\"Face {i+1}\", (x1, y1-10),\n",
        "                              font, 0.7, (255, 0, 0), 2)\n",
        "                except Exception:\n",
        "                    # Don't print error for better performance\n",
        "                    pass\n",
        "\n",
        "            # Convert to BGR for OpenCV operations\n",
        "            bgr_img = cv2.cvtColor(processed_img, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "            # Save frame directly to disk with a unique, sequential name\n",
        "            frame_path = os.path.join(self.frames_dir, f\"frame_{self.frame_count:05d}.jpg\")\n",
        "\n",
        "            # Resize to reduce file size (but keep reasonable quality)\n",
        "            resized_img = cv2.resize(bgr_img, (854, 480))  # 480p resolution\n",
        "\n",
        "            # Use moderate compression to save space\n",
        "            cv2.imwrite(frame_path, resized_img, [cv2.IMWRITE_JPEG_QUALITY, 90])\n",
        "\n",
        "            # Track this frame path\n",
        "            self.frame_paths.append(frame_path)\n",
        "            self.frames_saved += 1\n",
        "\n",
        "            # Periodically log progress\n",
        "            if self.frame_count % 10 == 0:\n",
        "                print(f\"Processed {self.frame_count} frames with {len(face_boxes)} faces in latest frame. Avg FPS: {fps:.1f}, Saved frames: {self.frames_saved}\")\n",
        "\n",
        "            # Increment frame counter\n",
        "            self.frame_count += 1\n",
        "\n",
        "        return True\n",
        "\n",
        "    def release(self):\n",
        "        \"\"\"Properly release resources when the application is shut down\"\"\"\n",
        "        import time\n",
        "        import os\n",
        "\n",
        "        # Calculate total elapsed time and average FPS\n",
        "        total_time = time.time() - self.start_time\n",
        "        avg_fps = self.frame_count / total_time if total_time > 0 else 0\n",
        "        print(f\"\\n\\n===== RELEASING RESOURCES =====\")\n",
        "        print(f\"Processed {self.frame_count} frames in {total_time:.2f} seconds ({avg_fps:.2f} FPS average)\")\n",
        "        print(f\"Saved {self.frames_saved} frames to disk\")\n",
        "\n",
        "        # Store the frame information for access after application completes\n",
        "        # This will be used by the post-application processing\n",
        "        global stored_frame_info\n",
        "        stored_frame_info = {\n",
        "            \"frames_dir\": self.frames_dir,\n",
        "            \"frame_count\": self.frames_saved,\n",
        "            \"output_video\": self.output_video_webm\n",
        "        }\n",
        "\n",
        "        # We'll let the post-application code handle video creation to avoid scheduler issues\n",
        "        print(\"Frame information stored for video creation after application completes\")\n",
        "\n",
        "        # Call base class release method\n",
        "        print(\"Calling base class release method...\")\n",
        "        super().release()\n",
        "        print(\"Base class release completed\")\n",
        "        print(\"===== RELEASE COMPLETED =====\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1098dd6-cce8-462a-83a7-a8478fb64c36",
      "metadata": {
        "id": "c1098dd6-cce8-462a-83a7-a8478fb64c36"
      },
      "source": [
        "## Application Class\n",
        "\n",
        "Now we will create the main application class to connect all the operators:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcb8b6f5-1952-45bc-b3b3-217e5d06d86c",
      "metadata": {
        "id": "bcb8b6f5-1952-45bc-b3b3-217e5d06d86c"
      },
      "outputs": [],
      "source": [
        "class PeopleAndFaceDetectApp(Application):\n",
        "    def __init__(self, data_path, model_path, *args, **kwargs):\n",
        "        \"\"\"Initialize the face and people detection application\"\"\"\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.name = \"People and Face Detection App\"\n",
        "        self.sample_data_path = data_path\n",
        "        self.model_path = model_path\n",
        "\n",
        "    def compose(self):\n",
        "        # Create resource allocator\n",
        "        pool = UnboundedAllocator(self, name=\"pool\")\n",
        "\n",
        "        # Configure and create the video source\n",
        "        replayer_args = self.kwargs(\"replayer_source\")\n",
        "        replayer_args[\"repeat\"] = False\n",
        "\n",
        "        source = VideoStreamReplayerOp(\n",
        "            self,\n",
        "            name=\"replayer_source\",\n",
        "            directory=self.sample_data_path,\n",
        "            **replayer_args\n",
        "        )\n",
        "\n",
        "        # Format converter for preprocessing\n",
        "        preprocessor_args = self.kwargs(\"preprocessor\")\n",
        "        format_converter = FormatConverterOp(\n",
        "            self,\n",
        "            name=\"preprocessor\",\n",
        "            pool=pool,\n",
        "            **preprocessor_args,\n",
        "        )\n",
        "\n",
        "        # Preprocessor for model input\n",
        "        preprocessor = PreprocessorOp(\n",
        "            self,\n",
        "            name=\"transpose\",\n",
        "            pool=pool,\n",
        "        )\n",
        "\n",
        "        # Inference operator\n",
        "        inference_args = self.kwargs(\"inference\")\n",
        "        inference_args[\"model_path_map\"] = {\n",
        "            \"face_detect\": os.path.join(self.sample_data_path, \"resnet34_peoplenet_int8.onnx\")\n",
        "        }\n",
        "        inference = InferenceOp(\n",
        "            self,\n",
        "            name=\"inference\",\n",
        "            allocator=pool,\n",
        "            **inference_args,\n",
        "        )\n",
        "\n",
        "        # Postprocessor for detection results\n",
        "        postprocessor_args = self.kwargs(\"postprocessor\")\n",
        "        postprocessor_args[\"image_width\"] = preprocessor_args[\"resize_width\"]\n",
        "        postprocessor_args[\"image_height\"] = preprocessor_args[\"resize_height\"]\n",
        "        postprocessor = PostprocessorOp(\n",
        "            self,\n",
        "            name=\"postprocessor\",\n",
        "            allocator=pool,\n",
        "            **postprocessor_args,\n",
        "        )\n",
        "\n",
        "        # Visualization sink\n",
        "        vis_args = self.kwargs(\"visualization\") or {}\n",
        "        visualization_sink = VisualizationSinkOp(\n",
        "            self,\n",
        "            name=\"visualization_sink\",\n",
        "            **vis_args\n",
        "        )\n",
        "\n",
        "        # Connect the operators\n",
        "        self.add_flow(source, format_converter)\n",
        "        self.add_flow(source, visualization_sink, {(\"output\", \"original_image\")})\n",
        "        self.add_flow(format_converter, preprocessor)\n",
        "        self.add_flow(preprocessor, inference, {(\"\", \"receivers\")})\n",
        "        self.add_flow(inference, postprocessor, {(\"transmitter\", \"in\")})\n",
        "        self.add_flow(postprocessor, visualization_sink, {(\"out\", \"in\")})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30ba97b8-8594-4643-9994-a69f32d5f35d",
      "metadata": {
        "id": "30ba97b8-8594-4643-9994-a69f32d5f35d"
      },
      "source": [
        "## Video Creation Helper Function\n",
        "\n",
        "We'll also define a function to create a video from the saved frames:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfa418bd-bb59-4841-bbc8-67f83d856dbd",
      "metadata": {
        "id": "cfa418bd-bb59-4841-bbc8-67f83d856dbd"
      },
      "outputs": [],
      "source": [
        "def create_video_from_frames(frames_dir, output_video_path):\n",
        "    \"\"\"Create a video file from a directory of frame images\"\"\"\n",
        "    print(f\"\\n\\n===== CREATING VIDEO FROM FRAMES =====\")\n",
        "\n",
        "    # Ensure the output directory exists\n",
        "    os.makedirs(os.path.dirname(output_video_path), exist_ok=True)\n",
        "\n",
        "    # Find all frame files\n",
        "    frame_pattern = os.path.join(frames_dir, \"frame_*.jpg\")\n",
        "    frame_paths = sorted(glob.glob(frame_pattern))\n",
        "    frame_count = len(frame_paths)\n",
        "\n",
        "    print(f\"Found {frame_count} frames in {frames_dir}\")\n",
        "\n",
        "    if frame_count == 0:\n",
        "        print(\"No frames found - cannot create video\")\n",
        "        return False\n",
        "\n",
        "    # Create a temporary file listing all frames\n",
        "    list_file = os.path.join(frames_dir, \"frames_list.txt\")\n",
        "    with open(list_file, 'w') as f:\n",
        "        for frame_path in frame_paths:\n",
        "            f.write(f\"file '{os.path.abspath(frame_path)}'\\n\")\n",
        "\n",
        "    print(f\"Created frames list file with {frame_count} entries at: {list_file}\")\n",
        "\n",
        "    # Create WebM with ffmpeg\n",
        "    webm_cmd = [\n",
        "        'ffmpeg',\n",
        "        '-y',                  # Overwrite output file if it exists\n",
        "        '-f', 'concat',        # Concatenate frames\n",
        "        '-safe', '0',          # Allow absolute paths\n",
        "        '-i', list_file,       # Input file list\n",
        "        '-c:v', 'libvpx',      # VP8 codec\n",
        "        '-b:v', '2M',          # Bitrate (2Mbps for good quality)\n",
        "        '-crf', '10',          # Quality factor (lower is better)\n",
        "        '-r', '30',            # Frame rate\n",
        "        output_video_path      # Output file\n",
        "    ]\n",
        "\n",
        "    print(f\"Running FFmpeg command: {' '.join(webm_cmd)}\")\n",
        "\n",
        "    result = subprocess.run(webm_cmd,\n",
        "                        stdout=subprocess.PIPE,\n",
        "                        stderr=subprocess.PIPE,\n",
        "                        text=True)\n",
        "\n",
        "    if result.returncode == 0:\n",
        "        if os.path.exists(output_video_path) and os.path.getsize(output_video_path) > 0:\n",
        "            print(f\"WebM created successfully: {output_video_path}\")\n",
        "            print(f\"WebM file size: {os.path.getsize(output_video_path) / (1024*1024):.2f} MB\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"ERROR: WebM file not created properly (missing or empty)\")\n",
        "            print(f\"FFmpeg stderr: {result.stderr}\")\n",
        "    else:\n",
        "        print(f\"WebM creation failed - error code: {result.returncode}\")\n",
        "        print(f\"FFmpeg error: {result.stderr}\")\n",
        "\n",
        "        # Try an alternative approach\n",
        "        print(\"Trying alternative approach using glob pattern...\")\n",
        "        alt_cmd = [\n",
        "            'ffmpeg',\n",
        "            '-y',\n",
        "            '-framerate', '30',\n",
        "            '-pattern_type', 'glob',\n",
        "            '-i', frame_pattern,\n",
        "            '-c:v', 'libvpx',\n",
        "            '-b:v', '2M',\n",
        "            output_video_path\n",
        "        ]\n",
        "\n",
        "        print(f\"Running alternative FFmpeg command: {' '.join(alt_cmd)}\")\n",
        "        result = subprocess.run(alt_cmd,\n",
        "                              stdout=subprocess.PIPE,\n",
        "                              stderr=subprocess.PIPE,\n",
        "                              text=True)\n",
        "\n",
        "        if result.returncode == 0:\n",
        "            print(f\"WebM created with alternative approach\")\n",
        "            print(f\"WebM file size: {os.path.getsize(output_video_path) / (1024*1024):.2f} MB\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"Alternative approach failed:\")\n",
        "            print(f\"FFmpeg stderr: {result.stderr}\")\n",
        "\n",
        "            # Try a third approach with OpenCV\n",
        "            try:\n",
        "                print(\"Trying third approach with OpenCV...\")\n",
        "\n",
        "                # Sort frames numerically to ensure correct order\n",
        "                sorted_frames = sorted(frame_paths,\n",
        "                                      key=lambda p: int(os.path.basename(p).split('_')[1].split('.')[0]))\n",
        "\n",
        "                # Load the first frame to get dimensions\n",
        "                frame = cv2.imread(sorted_frames[0])\n",
        "                height, width, _ = frame.shape\n",
        "\n",
        "                # Create a video writer\n",
        "                fourcc = cv2.VideoWriter_fourcc(*'VP80')  # WebM format\n",
        "                video_writer = cv2.VideoWriter(output_video_path,\n",
        "                                              fourcc,\n",
        "                                              30.0,  # FPS\n",
        "                                              (width, height))\n",
        "\n",
        "                # Add all frames to the video\n",
        "                for i, frame_path in enumerate(sorted_frames):\n",
        "                    frame = cv2.imread(frame_path)\n",
        "                    video_writer.write(frame)\n",
        "                    if i % 50 == 0:\n",
        "                        print(f\"Processing frame {i}/{len(sorted_frames)}\")\n",
        "\n",
        "                # Release the video writer\n",
        "                video_writer.release()\n",
        "\n",
        "                print(f\"Video created with OpenCV: {output_video_path}\")\n",
        "                print(f\"Video file size: {os.path.getsize(output_video_path) / (1024*1024):.2f} MB\")\n",
        "                return True\n",
        "            except Exception as e:\n",
        "                print(f\"OpenCV video creation failed: {e}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "\n",
        "    return False"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e617429-9080-4724-af64-f827b190d3e3",
      "metadata": {
        "id": "0e617429-9080-4724-af64-f827b190d3e3"
      },
      "source": [
        "## Running the Application\n",
        "\n",
        "Finally, we'll set up and run the application:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c78f03c-8f75-4a3d-8a26-02c8040a0253",
      "metadata": {
        "id": "5c78f03c-8f75-4a3d-8a26-02c8040a0253"
      },
      "outputs": [],
      "source": [
        "# Initialize global variable to store frame information\n",
        "stored_frame_info = None\n",
        "\n",
        "# Configure logging\n",
        "configure_logging()\n",
        "\n",
        "# Define paths\n",
        "script_dir = os.path.join(os.getcwd(), \"scripts/tao_peoplenet\")\n",
        "config_file = os.path.join(script_dir, \"tao_peoplenet.yaml\")\n",
        "data_path = os.path.join(script_dir, \"data/\")\n",
        "model_path = os.path.join(data_path, \"resnet34_peoplenet_int8.onnx\")\n",
        "\n",
        "# Create and configure the application\n",
        "app = PeopleAndFaceDetectApp(data_path, model_path)\n",
        "app.config(config_file)\n",
        "\n",
        "# Set scheduler\n",
        "scheduler = GreedyScheduler(app, name=\"greedy_scheduler\", max_duration_ms=100000000)\n",
        "app.scheduler(scheduler)\n",
        "\n",
        "# Run the application\n",
        "try:\n",
        "    print(\"\\n\\n===== STARTING APPLICATION =====\\n\\n\")\n",
        "    app.run()\n",
        "except Exception as e:\n",
        "    print(f\"Error running application: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "finally:\n",
        "    # Create video from frames after application completes\n",
        "    print(\"\\n\\n===== APPLICATION FINISHED =====\\n\\n\")\n",
        "    print(\"Waiting for video creation to complete...\")\n",
        "    time.sleep(5)  # Give a moment for any pending operations\n",
        "\n",
        "    if stored_frame_info:\n",
        "        frames_dir = stored_frame_info[\"frames_dir\"]\n",
        "        output_video = stored_frame_info[\"output_video\"]\n",
        "        create_video_from_frames(frames_dir, output_video)\n",
        "    else:\n",
        "        # Try fallback approach\n",
        "        frames_dir = os.path.join(script_dir, \"data/temp_frames\")\n",
        "        output_video = os.path.join(script_dir, \"data/people_faces_detected.webm\")\n",
        "\n",
        "        if os.path.exists(frames_dir):\n",
        "            print(f\"Using fallback approach to create video from {frames_dir}\")\n",
        "            create_video_from_frames(frames_dir, output_video)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Viewing the Original Video**\n",
        "\n",
        "First, let's make sure we are able to run the original \"un-processed\" video before running the \"Face Detection\" application. We will display the origina video using IPython's display capabilities:"
      ],
      "metadata": {
        "id": "lVrJ5arT38dg"
      },
      "id": "lVrJ5arT38dg"
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Video\n",
        "\n",
        "# Display the video with absolute path and optional width\n",
        "Video(\"/content/scripts/tao_peoplenet/data/people.mp4\", embed=True, width=800)"
      ],
      "metadata": {
        "id": "SJyEjqyQ4dYE"
      },
      "id": "SJyEjqyQ4dYE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "99c0867f-071d-4b01-9867-1e729a9242b9",
      "metadata": {
        "id": "99c0867f-071d-4b01-9867-1e729a9242b9"
      },
      "source": [
        "## **Viewing the Results**\n",
        "\n",
        "After running the application, we can view the resulting video using IPython's display capabilities:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6456262-ca7b-47f5-a0e3-8b09bcdfa4d1",
      "metadata": {
        "id": "f6456262-ca7b-47f5-a0e3-8b09bcdfa4d1"
      },
      "outputs": [],
      "source": [
        "# Display the detected faces video using absolute path\n",
        "Video(\"/content/scripts/tao_peoplenet/data/people_faces_detected.webm\", embed=True, width=800)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f4b8702-037e-40c7-a3cc-8d3421bbd0f6",
      "metadata": {
        "id": "1f4b8702-037e-40c7-a3cc-8d3421bbd0f6"
      },
      "source": [
        "This will display the processed video with face detections highlighted.\n",
        "\n",
        "## Summary\n",
        "\n",
        "In this section, we've built a complete Holoscan application for face detection using:\n",
        "\n",
        "1. **Pre-built operators** for video input, format conversion, and inference\n",
        "2. **Custom operators** for preprocessing, postprocessing, and visualization\n",
        "3. **GPU acceleration** throughout the pipeline\n",
        "4. **Configuration-driven approach** using YAML files\n",
        "5. **Video creation** from processed frames\n",
        "\n",
        "This application demonstrates the power of Holoscan for building high-performance, GPU-accelerated AI pipelines for real-time video processing."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b693c2f0-71b0-4960-b645-67ec1b9c99c6",
      "metadata": {
        "id": "b693c2f0-71b0-4960-b645-67ec1b9c99c6"
      },
      "source": [
        "# Conclusion and Next Steps\n",
        "\n",
        "## What We've Learned\n",
        "\n",
        "In this tutorial, we have explored NVIDIA Holoscan, a powerful SDK for building GPU-accelerated streaming AI pipelines. We have covered:\n",
        "\n",
        "1. **The Motivation for GPU Acceleration**\n",
        "   - Understanding CPU bottlenecks in AI workflows\n",
        "   - The benefits of end-to-end GPU acceleration\n",
        "   - How Holoscan addresses common data pipeline challenges\n",
        "\n",
        "2. **Core Holoscan Concepts**\n",
        "   - Operators as the basic building blocks of processing pipelines\n",
        "   - Data flow between operators through input and output ports\n",
        "   - Applications that orchestrate operators and data flow\n",
        "   - Configuring pipelines using YAML files\n",
        "\n",
        "3. **Building Custom Operators**\n",
        "   - Implementing the `setup` method to define ports\n",
        "   - Implementing the `compute` method for data processing\n",
        "   - Managing operator state across compute calls\n",
        "\n",
        "4. **Using Pre-Built Operators**\n",
        "   - Leveraging optimized operators for common tasks\n",
        "   - Configuring operators for specific requirements\n",
        "   - Connecting pre-built and custom operators\n",
        "\n",
        "5. **Building a Complete Application**\n",
        "   - Face detection using the PeopleNet model\n",
        "   - Video processing pipeline with multiple stages\n",
        "   - Real-time visualization and video output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5867797d-de93-49c0-85de-43ff27313477",
      "metadata": {
        "id": "5867797d-de93-49c0-85de-43ff27313477"
      },
      "source": [
        "## Key Takeaways\n",
        "\n",
        "- **GPU Acceleration**: Holoscan enables end-to-end GPU acceleration, significantly improving performance for streaming AI applications.\n",
        "- **Operator Paradigm**: The operator-based architecture provides a clean, modular approach to building complex pipelines.\n",
        "- **Configurability**: YAML-based configuration allows for flexible adjustment of parameters without code changes.\n",
        "- **Production-Ready**: Holoscan provides the tools needed to build high-performance, production-ready AI applications."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "deee367e-8757-441b-b155-4d9d0cd9c152",
      "metadata": {
        "id": "deee367e-8757-441b-b155-4d9d0cd9c152"
      },
      "source": [
        "## Next Steps\n",
        "\n",
        "To continue your journey with NVIDIA Holoscan, consider exploring these advanced topics:\n",
        "\n",
        "1. **Custom GPU Operators**: Develop operators that leverage CUDA for maximum performance.\n",
        "2. **Multi-GPU Processing**: Scale applications across multiple GPUs for higher throughput.\n",
        "3. **Edge Deployment**: Deploy Holoscan applications on edge devices with NVIDIA hardware.\n",
        "4. **Real-time Constraints**: Implement applications with strict latency requirements.\n",
        "5. **Custom Visualization**: Create advanced visualizations for specific domains."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be33aba6-1505-46f4-bf48-45737f4b1cd4",
      "metadata": {
        "id": "be33aba6-1505-46f4-bf48-45737f4b1cd4"
      },
      "source": [
        "## Resources\n",
        "\n",
        "- [NVIDIA Holoscan Documentation](https://docs.nvidia.com/holoscan/) - Comprehensive documentation and guides\n",
        "- [Holoscan GitHub Repository](https://github.com/nvidia-holoscan/holoscan-sdk) - Source code and examples\n",
        "- [NVIDIA Developer Forums](https://forums.developer.nvidia.com/) - Community support and discussions\n",
        "- [NGC Catalog](https://catalog.ngc.nvidia.com/) - Pre-trained models and containers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bea1289b-a108-45ab-98b7-a476a859f656",
      "metadata": {
        "id": "bea1289b-a108-45ab-98b7-a476a859f656"
      },
      "source": [
        "# Community Contribution: HoloHub\n",
        "\n",
        "## What is HoloHub?\n",
        "\n",
        "HoloHub is the official repository for community-contributed Holoscan applications and operators. It serves as a \"town-square\" where engineering teams can easily contribute, share, and reuse new functionalities while demonstrating novel applications.\n",
        "\n",
        "The repository provides:\n",
        "- A curated collection of sample applications across multiple domains\n",
        "- Reusable operators that accelerate development\n",
        "- Quality-graded code contributions with clear metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1020806f-5ec8-4aab-8f9c-558f01ee4543",
      "metadata": {
        "id": "1020806f-5ec8-4aab-8f9c-558f01ee4543"
      },
      "source": [
        "## How to Get Involved\n",
        "\n",
        "### Explore the Repository\n",
        "Visit the official HoloHub repository at: [https://github.com/nvidia-holoscan/holohub](https://github.com/nvidia-holoscan/holohub)\n",
        "\n",
        "### Contribute Your Work\n",
        "Consider contributing to the Holoscan ecosystem by:\n",
        "\n",
        "- **Sharing your custom operators** - Help others avoid reinventing the wheel\n",
        "- **Contributing applications** - Demonstrate innovative use cases\n",
        "- **Reporting bugs and requesting features** - Improve the platform for everyone\n",
        "- **Participating in community discussions** - Share knowledge and insights\n",
        "- **Creating tutorials and examples** - Help new users get started"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "731a46fd-856e-4f59-84be-cfaa7cdf81a6",
      "metadata": {
        "id": "731a46fd-856e-4f59-84be-cfaa7cdf81a6"
      },
      "source": [
        "### Contribution Guidelines\n",
        "\n",
        "All sample applications and operators in HoloHub are marked with a `metadata.json` file that:\n",
        "- Describes the contribution\n",
        "- Grades it for code quality\n",
        "- Marks supported compute platforms\n",
        "- Lists dependencies\n",
        "- Provides domain tags (e.g., medical, industrial, aerospace)\n",
        "\n",
        "Before contributing to HoloHub, please consult the contribution guidelines at: [https://github.com/nvidia-holoscan/holohub/blob/main/CONTRIBUTING.md](https://github.com/nvidia-holoscan/holohub/blob/main/CONTRIBUTING.md)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a1391e8-f70c-468a-8e59-fbf8d18042a9",
      "metadata": {
        "id": "0a1391e8-f70c-468a-8e59-fbf8d18042a9"
      },
      "source": [
        "## Benefits of Participating\n",
        "\n",
        "By contributing to HoloHub, you can:\n",
        "- Showcase your expertise to the Holoscan community\n",
        "- Learn from others' implementations\n",
        "- Accelerate your development by leveraging existing components\n",
        "- Connect with other developers working on similar challenges\n",
        "- Help shape the future of GPU-accelerated streaming AI applications\n",
        "\n",
        "**Thank you for exploring NVIDIA Holoscan! We hope this tutorial has equipped you with the knowledge and skills to build your own high-performance, GPU-accelerated streaming AI applications. Now, take the next step and become part of the growing Holoscan community through HoloHub!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dac6529-c1fd-411a-836b-2cba318b9e56",
      "metadata": {
        "id": "4dac6529-c1fd-411a-836b-2cba318b9e56"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}